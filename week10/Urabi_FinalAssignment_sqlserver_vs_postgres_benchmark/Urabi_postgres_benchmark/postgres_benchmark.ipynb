{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a8610a",
   "metadata": {},
   "source": [
    "# PostgreSQL benchmark notebook\n",
    "\n",
    "This notebook:\n",
    "1. Starts a PostgreSQL Docker container and mounts your CSV folder.\n",
    "2. Creates a database named 'bench'.\n",
    "3. Loads each CSV as a table with inferred types.\n",
    "4. Builds basic indexes and inferred foreign keys.\n",
    "5. Defines a read-heavy and write-heavy workload.\n",
    "6. Runs a concurrent benchmark and samples CPU and memory via `docker stats`.\n",
    "7. Reports throughput, p95 latency, average CPU and memory, and storage footprint.\n",
    "8. Writes results to `/mnt/data/pg_results.json` and `/mnt/data/pg_latency_detail.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec1336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility imports and setup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# install packages if missing\n",
    "def _pip_install(pkgs):\n",
    "    import importlib\n",
    "    to_install = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            to_install.append(pip_name)\n",
    "    if to_install:\n",
    "        import sys\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + to_install\n",
    "        print(\"Installing:\", \" \".join(to_install))\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "_pip_install([\n",
    "    (\"sqlalchemy\", \"sqlalchemy>=2.0.30\"),\n",
    "    (\"psycopg2\", \"psycopg2-binary>=2.9.9\"),\n",
    "    (\"pyodbc\", \"pyodbc>=5.1.0\"),\n",
    "    (\"pandas\", \"pandas>=2.2.2\"),\n",
    "    (\"pyarrow\", \"pyarrow>=16.1.0\"),\n",
    "])\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.types import Integer, BigInteger, Float, Boolean, DateTime, Text, String, DECIMAL\n",
    "\n",
    "def run_cmd(cmd_list, check=True, capture=True):\n",
    "    if capture:\n",
    "        res = subprocess.run(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)\n",
    "        if check and res.returncode != 0:\n",
    "            print(res.stdout)\n",
    "            print(res.stderr)\n",
    "            raise RuntimeError(f\"Command failed: {' '.join(cmd_list)}\")\n",
    "        return res.stdout.strip()\n",
    "    else:\n",
    "        res = subprocess.run(cmd_list, check=check)\n",
    "        return \"\"\n",
    "\n",
    "def assert_docker():\n",
    "    try:\n",
    "        out = run_cmd([\"docker\", \"--version\"])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Docker CLI not found. Install Docker Desktop and ensure 'docker' is on PATH.\") from e\n",
    "\n",
    "def safe_name(name):\n",
    "    base = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", name).strip(\"_\").lower()\n",
    "    if not base:\n",
    "        base = \"tbl_\" + ''.join(random.choices(string.ascii_lowercase, k=6))\n",
    "    if re.match(r\"^[0-9]\", base):\n",
    "        base = \"t_\" + base\n",
    "    return base\n",
    "\n",
    "def list_csvs(csv_dir):\n",
    "    p = Path(csv_dir)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV_DIR not found: {csv_dir}\")\n",
    "    files = [str(x) for x in p.glob(\"*.csv\")]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {csv_dir}\")\n",
    "    return files\n",
    "\n",
    "def infer_sqlalchemy_dtypes(sample_df, engine_kind):\n",
    "    dtype_map = {}\n",
    "    for col in sample_df.columns:\n",
    "        s = sample_df[col].dropna()\n",
    "        if s.empty:\n",
    "            dtype_map[col] = Text()\n",
    "            continue\n",
    "        if pd.api.types.is_integer_dtype(s):\n",
    "            try:\n",
    "                mx = int(s.max())\n",
    "                mn = int(s.min())\n",
    "                if mn < -2147483648 or mx > 2147483647:\n",
    "                    dtype_map[col] = BigInteger()\n",
    "                else:\n",
    "                    dtype_map[col] = Integer()\n",
    "            except Exception:\n",
    "                dtype_map[col] = BigInteger()\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            if engine_kind == \"mssql\":\n",
    "                dtype_map[col] = DECIMAL(38, 10)\n",
    "            else:\n",
    "                dtype_map[col] = Float()\n",
    "        elif pd.api.types.is_bool_dtype(s):\n",
    "            dtype_map[col] = Boolean()\n",
    "        elif pd.api.types.is_datetime64_any_dtype(s):\n",
    "            dtype_map[col] = DateTime()\n",
    "        else:\n",
    "            if engine_kind == \"mssql\":\n",
    "                dtype_map[col] = String(length=None)\n",
    "            else:\n",
    "                dtype_map[col] = Text()\n",
    "    return dtype_map\n",
    "\n",
    "def load_csv_to_sql(engine: Engine, table_name: str, csv_path: str, engine_kind: str, chunksize: int = 100_000):\n",
    "    it = pd.read_csv(csv_path, nrows=5000)\n",
    "    dt_cols = [c for c in it.columns if re.search(r\"(date|time|timestamp)$\", c, flags=re.I)]\n",
    "    if dt_cols:\n",
    "        it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
    "    dtype_map = infer_sqlalchemy_dtypes(it, engine_kind)\n",
    "\n",
    "    reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
    "    created = False\n",
    "    total_rows = 0\n",
    "    for i, chunk in enumerate(reader):\n",
    "        chunk.columns = [safe_name(c) for c in chunk.columns]\n",
    "        if not created:\n",
    "            chunk.to_sql(table_name, engine, if_exists=\"replace\", index=False, dtype=dtype_map, method=None)\n",
    "            created = True\n",
    "        else:\n",
    "            chunk.to_sql(table_name, engine, if_exists=\"append\", index=False, method=None)\n",
    "        total_rows += len(chunk)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Loaded {total_rows} rows into {table_name} ...\")\n",
    "    if not created:\n",
    "        df = pd.read_csv(csv_path, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
    "        df.columns = [safe_name(c) for c in df.columns]\n",
    "        df.to_sql(table_name, engine, if_exists=\"replace\", index=False, dtype=infer_sqlalchemy_dtypes(df, engine_kind))\n",
    "        total_rows = len(df)\n",
    "    return total_rows\n",
    "\n",
    "def classify_tables(table_names):\n",
    "    fact = None\n",
    "    dims = []\n",
    "    for t in table_names:\n",
    "        if re.search(r\"(^|_)fact($|_)\", t):\n",
    "            fact = t\n",
    "        else:\n",
    "            dims.append(t)\n",
    "    if fact is None and table_names:\n",
    "        fact = sorted(table_names)[0]\n",
    "    return fact, dims\n",
    "\n",
    "def inspect_schema(engine: Engine, schema=None):\n",
    "    insp = sa.inspect(engine)\n",
    "    tables = insp.get_table_names(schema=schema)\n",
    "    info = {}\n",
    "    for t in tables:\n",
    "        cols = insp.get_columns(t, schema=schema)\n",
    "        info[t] = {\n",
    "            \"columns\": cols,\n",
    "            \"pk\": [c[\"name\"] for c in cols if c.get(\"primary_key\", False)]\n",
    "        }\n",
    "    return info\n",
    "\n",
    "def guess_roles(engine: Engine, table: str):\n",
    "    insp = sa.inspect(engine)\n",
    "    cols = [c[\"name\"] for c in insp.get_columns(table)]\n",
    "    id_cols = [c for c in cols if c == \"id\" or c.endswith(\"_id\")]\n",
    "    dt_cols = []\n",
    "    for c in cols:\n",
    "        try:\n",
    "            typ = insp.get_columns(table, c)[0][\"type\"]\n",
    "        except Exception:\n",
    "            typ = None\n",
    "        if re.search(r\"(date|time|timestamp)$\", c):\n",
    "            dt_cols.append(c)\n",
    "    if not dt_cols:\n",
    "        dt_cols = [c for c in cols if re.search(r\"(date|time)\", c)]\n",
    "    measure_cols = []\n",
    "    for c in cols:\n",
    "        if c in id_cols:\n",
    "            continue\n",
    "        try:\n",
    "            typ = insp.get_columns(table, c)[0][\"type\"]\n",
    "            text_typ = str(typ).lower()\n",
    "            if any(k in text_typ for k in [\"int\", \"bigint\", \"float\", \"double\", \"numeric\", \"decimal\"]):\n",
    "                measure_cols.append(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "    id_cols = id_cols[:5]\n",
    "    dt_col = dt_cols[0] if dt_cols else None\n",
    "    measure_col = measure_cols[0] if measure_cols else None\n",
    "    return {\"id_cols\": id_cols, \"dt_col\": dt_col, \"measure_col\": measure_col, \"all_cols\": cols}\n",
    "\n",
    "def create_indexes_and_fks(engine: Engine, fact: str, dims: list, dialect: str):\n",
    "    insp = sa.inspect(engine)\n",
    "    with engine.begin() as conn:\n",
    "        for d in dims:\n",
    "            dcols = [c[\"name\"] for c in insp.get_columns(d)]\n",
    "            if \"id\" in dcols:\n",
    "                fk_col = f\"{d}_id\"\n",
    "                fcols = [c[\"name\"] for c in insp.get_columns(fact)]\n",
    "                if fk_col in fcols:\n",
    "                    idx_name = f\"ix_{fact}_{fk_col}\"\n",
    "                    try:\n",
    "                        conn.execute(text(f\"CREATE INDEX IF NOT EXISTS {idx_name} ON {fact} ({fk_col})\"))\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            conn.execute(text(f\"IF NOT EXISTS (SELECT name FROM sys.indexes WHERE name = '{idx_name}') CREATE INDEX {idx_name} ON {fact} ({fk_col})\"))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Index creation skipped for {fk_col}: {e}\")\n",
    "                    fk_name = f\"fk_{fact}_{fk_col}_{d}_id\"\n",
    "                    try:\n",
    "                        conn.execute(text(f\"ALTER TABLE {fact} ADD CONSTRAINT {fk_name} FOREIGN KEY ({fk_col}) REFERENCES {d}(id)\"))\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        roles = guess_roles(engine, fact)\n",
    "        if roles[\"dt_col\"]:\n",
    "            idxdt = f\"ix_{fact}_{roles['dt_col']}\"\n",
    "            try:\n",
    "                conn.execute(text(f\"CREATE INDEX IF NOT EXISTS {idxdt} ON {fact} ({roles['dt_col']})\"))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    conn.execute(text(f\"IF NOT EXISTS (SELECT name FROM sys.indexes WHERE name = '{idxdt}') CREATE INDEX {idxdt} ON {fact} ({roles['dt_col']})\"))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def monitor_docker_stats(container_name, stop_event, interval=1.0):\n",
    "    samples = []\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            out = run_cmd([\"docker\", \"stats\", container_name, \"--no-stream\", \"--format\", \"{{json .}}\"], check=False)\n",
    "            if out.strip():\n",
    "                try:\n",
    "                    rec = json.loads(out.strip())\n",
    "                    cpu = float(str(rec.get(\"CPUPerc\",\"0\")).strip(\"%\"))\n",
    "                    mem_perc = float(str(rec.get(\"MemPerc\",\"0\")).strip(\"%\"))\n",
    "                    mem_usage = rec.get(\"MemUsage\",\"0 / 0\")\n",
    "                    samples.append({\"ts\": time.time(), \"cpu_percent\": cpu, \"mem_percent\": mem_perc, \"mem_usage_raw\": mem_usage})\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(interval)\n",
    "    return samples\n",
    "\n",
    "def p95(values):\n",
    "    if not values:\n",
    "        return None\n",
    "    s = sorted(values)\n",
    "    k = int(math.ceil(0.95 * len(s))) - 1\n",
    "    return s[max(0, min(k, len(s)-1))]\n",
    "\n",
    "def summarise_stats(samples):\n",
    "    if not samples:\n",
    "        return {\"avg_cpu_percent\": None, \"avg_mem_percent\": None, \"p95_cpu_percent\": None, \"p95_mem_percent\": None}\n",
    "    cpus = [x[\"cpu_percent\"] for x in samples if x.get(\"cpu_percent\") is not None]\n",
    "    mems = [x[\"mem_percent\"] for x in samples if x.get(\"mem_percent\") is not None]\n",
    "    return {\n",
    "        \"avg_cpu_percent\": sum(cpus)/len(cpus) if cpus else None,\n",
    "        \"avg_mem_percent\": sum(mems)/len(mems) if mems else None,\n",
    "        \"p95_cpu_percent\": p95(cpus) if cpus else None,\n",
    "        \"p95_mem_percent\": p95(mems) if mems else None,\n",
    "        \"num_samples\": len(samples)\n",
    "    }\n",
    "\n",
    "class WorkloadRunner:\n",
    "    def __init__(self, engine: Engine, dialect: str, fact: str, dims: list, duration_sec: int = 60, concurrency: int = 8, read_ratio: float = 0.8):\n",
    "        self.engine = engine\n",
    "        self.dialect = dialect\n",
    "        self.fact = fact\n",
    "        self.dims = dims\n",
    "        self.duration_sec = duration_sec\n",
    "        self.concurrency = concurrency\n",
    "        self.read_ratio = read_ratio\n",
    "        self.roles = guess_roles(engine, fact)\n",
    "        self._stop_time = time.time() + duration_sec\n",
    "        self.latencies = []\n",
    "        self.errors = 0\n",
    "        self.completed = 0\n",
    "\n",
    "    def _sql_now(self):\n",
    "        return \"CURRENT_TIMESTAMP\" if self.dialect == \"postgresql\" else \"SYSDATETIME()\"\n",
    "\n",
    "    def _month_trunc(self, col):\n",
    "        if self.dialect == \"postgresql\":\n",
    "            return f\"date_trunc('month', {col})\"\n",
    "        else:\n",
    "            return f\"DATETRUNC(month, {col})\"\n",
    "\n",
    "    def _random_read_query(self):\n",
    "        m = self.roles[\"measure_col\"]\n",
    "        dt = self.roles[\"dt_col\"]\n",
    "        id_cols = self.roles[\"id_cols\"]\n",
    "        if m and dt:\n",
    "            return f\"SELECT {self._month_trunc(dt)} AS m, SUM({m}) AS s FROM {self.fact} GROUP BY {self._month_trunc(dt)} ORDER BY m DESC OFFSET 0 ROWS FETCH NEXT 50 ROWS ONLY\" if self.dialect == \"mssql\" else f\"SELECT {self._month_trunc(dt)} AS m, SUM({m}) AS s FROM {self.fact} GROUP BY {self._month_trunc(dt)} ORDER BY m DESC LIMIT 50\"\n",
    "        if id_cols:\n",
    "            fk = id_cols[0]\n",
    "            match_dim = None\n",
    "            for d in self.dims:\n",
    "                if fk == f\"{d}_id\":\n",
    "                    match_dim = d\n",
    "                    break\n",
    "            if match_dim:\n",
    "                return f\"SELECT d.id, COUNT(*) AS c FROM {self.fact} f JOIN {match_dim} d ON f.{fk} = d.id GROUP BY d.id ORDER BY c DESC\"\n",
    "        return f\"SELECT COUNT(*) FROM {self.fact}\"\n",
    "\n",
    "    def _random_write_query(self):\n",
    "        id_cols = self.roles[\"id_cols\"]\n",
    "        dt = self.roles[\"dt_col\"]\n",
    "        m = self.roles[\"measure_col\"]\n",
    "        cols = self.roles[\"all_cols\"]\n",
    "        if id_cols and m:\n",
    "            with self.engine.begin() as conn:\n",
    "                fk = id_cols[0]\n",
    "                try:\n",
    "                    keys = conn.execute(text(f\"SELECT {fk} FROM {self.fact} WHERE {fk} IS NOT NULL ORDER BY {self._dialect_random()} LIMIT 1000\" if self.dialect==\"postgresql\" else f\"SELECT TOP 1000 {fk} FROM {self.fact} WHERE {fk} IS NOT NULL ORDER BY NEWID()\")).fetchall()\n",
    "                    key = keys[random.randrange(len(keys))][0] if keys else None\n",
    "                except Exception:\n",
    "                    key = None\n",
    "            val_m = round(random.random() * 1000, 6)\n",
    "            cols_in = []\n",
    "            vals_in = []\n",
    "            if id_cols and key is not None:\n",
    "                cols_in.append(fk)\n",
    "                vals_in.append(str(key))\n",
    "            if dt:\n",
    "                cols_in.append(dt)\n",
    "                vals_in.append(self._now_literal())\n",
    "            if m:\n",
    "                cols_in.append(m)\n",
    "                vals_in.append(str(val_m))\n",
    "            if not cols_in:\n",
    "                cols_in = [m]\n",
    "                vals_in = [str(val_m)]\n",
    "            return f\"INSERT INTO {self.fact} ({', '.join(cols_in)}) VALUES ({', '.join(vals_in)})\"\n",
    "        target_col = None\n",
    "        with self.engine.begin() as conn:\n",
    "            for c in cols:\n",
    "                if c not in id_cols and c != dt:\n",
    "                    target_col = c\n",
    "                    break\n",
    "        if target_col:\n",
    "            return f\"UPDATE {self.fact} SET {target_col} = {target_col} WHERE 1=0\"\n",
    "        return f\"SELECT 1\"\n",
    "\n",
    "    def _now_literal(self):\n",
    "        return self._sql_now()\n",
    "\n",
    "    def _dialect_random(self):\n",
    "        return \"random()\" if self.dialect == \"postgresql\" else \"NEWID()\"\n",
    "\n",
    "    def _exec(self, sql: str):\n",
    "        t0 = time.perf_counter()\n",
    "        ok = True\n",
    "        err = \"\"\n",
    "        try:\n",
    "            with self.engine.begin() as conn:\n",
    "                conn.execute(text(sql))\n",
    "        except Exception as e:\n",
    "            ok = False\n",
    "            err = str(e)[:200]\n",
    "        dt = time.perf_counter() - t0\n",
    "        self.latencies.append({\"ts\": time.time(), \"sql\": sql, \"ok\": ok, \"latency_ms\": dt * 1000.0, \"dialect\": self.dialect})\n",
    "        if ok:\n",
    "            self.completed += 1\n",
    "        else:\n",
    "            self.errors += 1\n",
    "\n",
    "    def _worker(self):\n",
    "        rng = random.Random()\n",
    "        while time.time() < self._stop_time:\n",
    "            q = self._random_read_query() if rng.random() < self.read_ratio else self._random_write_query()\n",
    "            self._exec(q)\n",
    "\n",
    "    def run(self):\n",
    "        stop_event = threading.Event()\n",
    "        self._mon_samples = []\n",
    "        def mon_wrapper():\n",
    "            self._mon_samples.extend(monitor_docker_stats(self._container_name, stop_event, 1.0))\n",
    "        mon_thr = threading.Thread(target=mon_wrapper, daemon=True)\n",
    "        mon_thr.start()\n",
    "        t_start = time.time()\n",
    "        with ThreadPoolExecutor(max_workers=self.concurrency) as ex:\n",
    "            futs = [ex.submit(self._worker) for _ in range(self.concurrency)]\n",
    "            for f in as_completed(futs):\n",
    "                pass\n",
    "        stop_event.set()\n",
    "        mon_thr.join(timeout=2.0)\n",
    "        t_end = time.time()\n",
    "        self.elapsed = t_end - t_start\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def _container_name(self):\n",
    "        return \"pg_bench\" if self.dialect == \"postgresql\" else \"sqlserver_bench\"\n",
    "\n",
    "    def summary(self):\n",
    "        lats_ok = [x[\"latency_ms\"] for x in self.latencies if x[\"ok\"]]\n",
    "        return {\n",
    "            \"transactions\": self.completed,\n",
    "            \"errors\": self.errors,\n",
    "            \"elapsed_sec\": self.elapsed,\n",
    "            \"throughput_tps\": (self.completed / self.elapsed) if self.elapsed > 0 else None,\n",
    "            \"p95_latency_ms\": p95(lats_ok) if lats_ok else None,\n",
    "            \"cpu_mem_summary\": summarise_stats(self._mon_samples)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a8ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "CSV_DIR = r\"C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\"\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"postgres\"\n",
    "PG_DB = \"bench\"\n",
    "PG_PORT = 55432\n",
    "PG_CONTAINER = \"pg_bench\"\n",
    "PG_IMAGE = \"postgres:16\"\n",
    "DATA_VOLUME = \"pg_bench_data\"\n",
    "\n",
    "assert_docker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc98b516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV_DIR: C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\n",
      "Running: docker run -d --name pg_bench -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=postgres -p 55432:5432 -v pg_bench_data:/var/lib/postgresql/data -v C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV:/csv postgres:16\n",
      "Port ready: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start PostgreSQL container\n",
    "mount_src = os.path.abspath(CSV_DIR)\n",
    "print(\"CSV_DIR:\", mount_src)\n",
    "existing = run_cmd([\"docker\", \"ps\", \"-a\", \"--format\", \"{{.Names}}\"])\n",
    "if PG_CONTAINER in existing.splitlines():\n",
    "    state = run_cmd([\"docker\", \"inspect\", \"-f\", \"{{.State.Status}}\", PG_CONTAINER])\n",
    "    if state != \"running\":\n",
    "        print(\"Removing existing container:\", PG_CONTAINER)\n",
    "        run_cmd([\"docker\", \"rm\", \"-f\", PG_CONTAINER], check=False)\n",
    "\n",
    "run_cmd([\"docker\", \"pull\", PG_IMAGE])\n",
    "cmd = [\n",
    "    \"docker\", \"run\", \"-d\",\n",
    "    \"--name\", PG_CONTAINER,\n",
    "    \"-e\", f\"POSTGRES_USER={PG_USER}\",\n",
    "    \"-e\", f\"POSTGRES_PASSWORD={PG_PASSWORD}\",\n",
    "    \"-e\", \"POSTGRES_DB=postgres\",\n",
    "    \"-p\", f\"{PG_PORT}:5432\",\n",
    "    \"-v\", f\"{DATA_VOLUME}:/var/lib/postgresql/data\",\n",
    "    \"-v\", f\"{mount_src}:/csv\",\n",
    "    PG_IMAGE\n",
    "]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "run_cmd(cmd)\n",
    "time.sleep(2)\n",
    "\n",
    "def wait_port(host, port, timeout=90):\n",
    "    t0 = time.time()\n",
    "    while time.time() - t0 < timeout:\n",
    "        s = socket.socket()\n",
    "        s.settimeout(2.0)\n",
    "        try:\n",
    "            s.connect((host, port))\n",
    "            s.close()\n",
    "            return True\n",
    "        except Exception:\n",
    "            time.sleep(1.0)\n",
    "    return False\n",
    "\n",
    "ok = wait_port(\"127.0.0.1\", PG_PORT, 90)\n",
    "print(\"Port ready:\", ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57b82e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: bench\n",
      "Connected to: postgresql+psycopg2://postgres:***@127.0.0.1:55432/bench\n"
     ]
    }
   ],
   "source": [
    "# Connect to Postgres and create bench database (autocommit-safe)\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "admin_url = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=PG_USER,\n",
    "    password=PG_PASSWORD,\n",
    "    host=\"127.0.0.1\",\n",
    "    port=PG_PORT,\n",
    "    database=\"postgres\",\n",
    ")\n",
    "admin_engine = sa.create_engine(admin_url, pool_pre_ping=True, future=True)\n",
    "\n",
    "# Check existence using a normal transaction\n",
    "with admin_engine.begin() as conn:\n",
    "    exists = conn.execute(\n",
    "        text(\"SELECT 1 FROM pg_database WHERE datname = :d\"),\n",
    "        {\"d\": PG_DB},\n",
    "    ).scalar()\n",
    "\n",
    "# Create the database outside a transaction\n",
    "if not exists:\n",
    "    with admin_engine.connect() as conn:\n",
    "        conn = conn.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "        conn.execute(text(f'CREATE DATABASE \"{PG_DB}\"'))\n",
    "        print(\"Created database:\", PG_DB)\n",
    "else:\n",
    "    print(\"Database exists:\", PG_DB)\n",
    "\n",
    "# Connect to the target database\n",
    "bench_url = URL.create(\n",
    "    drivername=\"postgresql+psycopg2\",\n",
    "    username=PG_USER,\n",
    "    password=PG_PASSWORD,\n",
    "    host=\"127.0.0.1\",\n",
    "    port=PG_PORT,\n",
    "    database=PG_DB,\n",
    ")\n",
    "engine = sa.create_engine(bench_url, pool_pre_ping=True, future=True)\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"SELECT current_database()\"))\n",
    "print(\"Connected to:\", bench_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cdd5b4-1625-4716-bbdb-b2b222b11c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine ready.\n"
     ]
    }
   ],
   "source": [
    "# Ensure a live SQLAlchemy engine to the 'bench' DB exists\n",
    "try:\n",
    "    engine  # noqa: F821\n",
    "except NameError:\n",
    "    from sqlalchemy.engine.url import URL\n",
    "    bench_url = URL.create(\n",
    "        drivername=\"postgresql+psycopg2\",\n",
    "        username=PG_USER,\n",
    "        password=PG_PASSWORD,\n",
    "        host=\"127.0.0.1\",\n",
    "        port=PG_PORT,\n",
    "        database=PG_DB,\n",
    "    )\n",
    "    engine = sa.create_engine(bench_url, pool_pre_ping=True, future=True)\n",
    "\n",
    "# Quick sanity check\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "print(\"Engine ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f52598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\campaign_desc.csv -> table campaign_desc\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\campaign_table.csv -> table campaign_table\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\causal_data.csv -> table causal_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 rows into causal_data ...\n",
      "Loaded 4000000 rows into causal_data ...\n",
      "Loaded 6000000 rows into causal_data ...\n",
      "Loaded 8000000 rows into causal_data ...\n",
      "Loaded 10000000 rows into causal_data ...\n",
      "Loaded 12000000 rows into causal_data ...\n",
      "Loaded 14000000 rows into causal_data ...\n",
      "Loaded 16000000 rows into causal_data ...\n",
      "Loaded 18000000 rows into causal_data ...\n",
      "Loaded 20000000 rows into causal_data ...\n",
      "Loaded 22000000 rows into causal_data ...\n",
      "Loaded 24000000 rows into causal_data ...\n",
      "Loaded 26000000 rows into causal_data ...\n",
      "Loaded 28000000 rows into causal_data ...\n",
      "Loaded 30000000 rows into causal_data ...\n",
      "Loaded 32000000 rows into causal_data ...\n",
      "Loaded 34000000 rows into causal_data ...\n",
      "Loaded 36000000 rows into causal_data ...\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\coupon.csv -> table coupon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\coupon_redempt.csv -> table coupon_redempt\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\hh_demographic.csv -> table hh_demographic\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\product.csv -> table product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\transaction_data.csv -> table transaction_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:119: FutureWarning: The 'keep_date_col' keyword in pd.read_csv is deprecated and will be removed in a future version. Explicitly remove unwanted columns after parsing instead.\n",
      "  it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:119: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:119: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 rows into transaction_data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tables: {'campaign_desc': 30, 'campaign_table': 7208, 'causal_data': 36786524, 'coupon': 124548, 'coupon_redempt': 2318, 'hh_demographic': 801, 'product': 92353, 'transaction_data': 2595732}\n",
      "Load time seconds: 644.26\n",
      "Fact table: campaign_desc\n",
      "Dim tables: ['campaign_desc', 'campaign_table', 'causal_data', 'coupon', 'coupon_redempt', 'hh_demographic', 'product', 'transaction_data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load all CSVs into tables\n",
    "csvs = list_csvs(CSV_DIR)\n",
    "table_map = {}\n",
    "rows_loaded = {}\n",
    "t0 = time.time()\n",
    "for f in csvs:\n",
    "    name = safe_name(Path(f).stem)\n",
    "    table_map[name] = f\n",
    "    print(f\"Loading {f} -> table {name}\")\n",
    "    n = load_csv_to_sql(engine, name, f, \"postgresql\", chunksize=200_000)\n",
    "    rows_loaded[name] = n\n",
    "t_load = time.time() - t0\n",
    "print(\"Loaded tables:\", rows_loaded)\n",
    "print(f\"Load time seconds: {t_load:.2f}\")\n",
    "\n",
    "fact, dims = classify_tables(list(table_map.keys()))\n",
    "print(\"Fact table:\", fact)\n",
    "print(\"Dim tables:\", dims)\n",
    "\n",
    "create_indexes_and_fks(engine, fact, dims, \"postgresql\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3054a0-08b0-4602-9dc0-8bb24d223a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: C:\\Users\\uhati\\Desktop\\Project phase 2\\results\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\uhati\\Desktop\\Project phase 2\\results\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PG_LAT_PATH = OUTPUT_DIR / \"pg_latency_detail.csv\"\n",
    "PG_JSON_PATH = OUTPUT_DIR / \"pg_results.json\"\n",
    "print(\"Results will be saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f089a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema roles before: {'id_cols': [], 'dt_col': None, 'measure_col': None, 'all_cols': ['description', 'campaign', 'start_day', 'end_day']}\n",
      "Schema roles now: {'id_cols': [], 'dt_col': None, 'measure_col': None, 'all_cols': ['description', 'campaign', 'start_day', 'end_day']}\n",
      "Benchmark summary: {\n",
      "  \"transactions\": 172258,\n",
      "  \"errors\": 0,\n",
      "  \"elapsed_sec\": 61.97706627845764,\n",
      "  \"throughput_tps\": 2779.3829289379332,\n",
      "  \"p95_latency_ms\": 3.173099976265803,\n",
      "  \"cpu_mem_summary\": {\n",
      "    \"avg_cpu_percent\": null,\n",
      "    \"avg_mem_percent\": null,\n",
      "    \"p95_cpu_percent\": null,\n",
      "    \"p95_mem_percent\": null\n",
      "  }\n",
      "}\n",
      "Latency details: C:\\Users\\uhati\\Desktop\\Project phase 2\\results\\pg_latency_detail.csv\n",
      "Saved results: C:\\Users\\uhati\\Desktop\\Project phase 2\\results\\pg_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_45888\\2678920223.py:74: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    }
   ],
   "source": [
    "# Define and run the workload\n",
    "# Inspect schema and roles\n",
    "info = inspect_schema(engine)\n",
    "roles = guess_roles(engine, fact)\n",
    "print(\"Schema roles before:\", roles)\n",
    "\n",
    "# If no timestamp or numeric measure detected, promote common date-like text columns\n",
    "if roles[\"dt_col\"] is None:\n",
    "    with engine.begin() as conn:\n",
    "        insp = sa.inspect(engine)\n",
    "        for t in insp.get_table_names():\n",
    "            cols = {c[\"name\"]: c for c in insp.get_columns(t)}\n",
    "            candidates = [\n",
    "                \"start_day\",\"end_day\",\"start_date\",\"end_date\",\n",
    "                \"created_at\",\"updated_at\",\"timestamp\",\"event_time\",\"date\",\"datetime\"\n",
    "            ]\n",
    "            for col in candidates:\n",
    "                if col in cols and \"text\" in str(cols[col][\"type\"]).lower():\n",
    "                    try:\n",
    "                        conn.execute(text(\n",
    "                            f\"ALTER TABLE {t} ALTER COLUMN {col} TYPE timestamp USING NULLIF({col}, '')::timestamp\"\n",
    "                        ))\n",
    "                        print(f\"Converted {t}.{col} to timestamp\")\n",
    "                    except Exception:\n",
    "                        # If conversion fails, skip silently\n",
    "                        pass\n",
    "    # Refresh roles after attempted promotion\n",
    "    roles = guess_roles(engine, fact)\n",
    "\n",
    "print(\"Schema roles now:\", roles)\n",
    "\n",
    "# Workload configuration\n",
    "DURATION_SEC = 60\n",
    "CONCURRENCY = 8\n",
    "READ_RATIO = 0.8\n",
    "\n",
    "# Run the benchmark\n",
    "runner = WorkloadRunner(engine, \"postgresql\", fact, dims,\n",
    "                        duration_sec=DURATION_SEC,\n",
    "                        concurrency=CONCURRENCY,\n",
    "                        read_ratio=READ_RATIO)\n",
    "runner.run()\n",
    "summary = runner.summary()\n",
    "print(\"Benchmark summary:\", json.dumps(summary, indent=2))\n",
    "\n",
    "# Save latency details\n",
    "lat_df = pd.DataFrame(runner.latencies)\n",
    "lat_df.to_csv(PG_LAT_PATH, index=False)\n",
    "print(\"Latency details:\", PG_LAT_PATH)\n",
    "\n",
    "# Storage footprint\n",
    "with engine.begin() as conn:\n",
    "    db_bytes = conn.execute(text(\"SELECT pg_database_size(:d)\"), {\"d\": PG_DB}).scalar()\n",
    "    per_table = conn.execute(text(\"\"\"\n",
    "        SELECT relname AS table_name, pg_total_relation_size(relid) AS bytes\n",
    "        FROM pg_catalog.pg_statio_user_tables\n",
    "        ORDER BY pg_total_relation_size(relid) DESC\n",
    "    \"\"\")).mappings().all()\n",
    "store = {\n",
    "    \"database_bytes\": int(db_bytes) if db_bytes is not None else None,\n",
    "    \"tables\": [{\"table\": r[\"table_name\"], \"bytes\": int(r[\"bytes\"])} for r in per_table]\n",
    "}\n",
    "\n",
    "# Results package\n",
    "results = {\n",
    "    \"engine\": \"postgresql\",\n",
    "    \"version\": run_cmd([\"docker\", \"exec\", PG_CONTAINER, \"psql\", \"-U\", PG_USER, \"-t\", \"-c\", \"SHOW server_version;\"], check=False).strip(),\n",
    "    \"rows_loaded\": rows_loaded,\n",
    "    \"load_time_sec\": t_load,\n",
    "    \"workload\": {\"duration_sec\": DURATION_SEC, \"concurrency\": CONCURRENCY, \"read_ratio\": READ_RATIO},\n",
    "    \"bench_summary\": summary,\n",
    "    \"storage\": store,\n",
    "    \"admin_time_sec\": t_load,\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "}\n",
    "\n",
    "with open(PG_JSON_PATH, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Saved results:\", PG_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a014174-67ea-4e5b-b5a8-dbce34905512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
