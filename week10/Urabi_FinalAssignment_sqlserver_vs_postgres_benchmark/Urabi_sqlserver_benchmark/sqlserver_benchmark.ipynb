{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327e3833",
   "metadata": {},
   "source": [
    "# SQL Server benchmark notebook\n",
    "\n",
    "This notebook:\n",
    "1. Starts a SQL Server 2022 Docker container and mounts your CSV folder.\n",
    "2. Creates a database named 'Bench'.\n",
    "3. Loads each CSV as a table with inferred types.\n",
    "4. Builds basic indexes and inferred foreign keys.\n",
    "5. Defines a read-heavy and write-heavy workload.\n",
    "6. Runs a concurrent benchmark and samples CPU and memory via `docker stats`.\n",
    "7. Reports throughput, p95 latency, average CPU and memory, and storage footprint.\n",
    "8. Writes results to `/mnt/data/mssql_results.json` and `/mnt/data/mssql_latency_detail.csv`.\n",
    "9. Optionally checks functional parity with PostgreSQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de39841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility imports and setup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import socket\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# install packages if missing\n",
    "def _pip_install(pkgs):\n",
    "    import importlib\n",
    "    to_install = []\n",
    "    for mod, pip_name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(mod)\n",
    "        except Exception:\n",
    "            to_install.append(pip_name)\n",
    "    if to_install:\n",
    "        import sys\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + to_install\n",
    "        print(\"Installing:\", \" \".join(to_install))\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "_pip_install([\n",
    "    (\"sqlalchemy\", \"sqlalchemy>=2.0.30\"),\n",
    "    (\"psycopg2\", \"psycopg2-binary>=2.9.9\"),\n",
    "    (\"pyodbc\", \"pyodbc>=5.1.0\"),\n",
    "    (\"pandas\", \"pandas>=2.2.2\"),\n",
    "    (\"pyarrow\", \"pyarrow>=16.1.0\"),\n",
    "])\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.types import Integer, BigInteger, Float, Boolean, DateTime, Text, String, DECIMAL\n",
    "\n",
    "def run_cmd(cmd_list, check=True, capture=True):\n",
    "    if capture:\n",
    "        res = subprocess.run(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)\n",
    "        if check and res.returncode != 0:\n",
    "            print(res.stdout)\n",
    "            print(res.stderr)\n",
    "            raise RuntimeError(f\"Command failed: {' '.join(cmd_list)}\")\n",
    "        return res.stdout.strip()\n",
    "    else:\n",
    "        res = subprocess.run(cmd_list, check=check)\n",
    "        return \"\"\n",
    "\n",
    "def assert_docker():\n",
    "    try:\n",
    "        out = run_cmd([\"docker\", \"--version\"])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Docker CLI not found. Install Docker Desktop and ensure 'docker' is on PATH.\") from e\n",
    "\n",
    "def safe_name(name):\n",
    "    base = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", name).strip(\"_\").lower()\n",
    "    if not base:\n",
    "        base = \"tbl_\" + ''.join(random.choices(string.ascii_lowercase, k=6))\n",
    "    if re.match(r\"^[0-9]\", base):\n",
    "        base = \"t_\" + base\n",
    "    return base\n",
    "\n",
    "def list_csvs(csv_dir):\n",
    "    p = Path(csv_dir)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV_DIR not found: {csv_dir}\")\n",
    "    files = [str(x) for x in p.glob(\"*.csv\")]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in: {csv_dir}\")\n",
    "    return files\n",
    "\n",
    "def infer_sqlalchemy_dtypes(sample_df, engine_kind):\n",
    "    dtype_map = {}\n",
    "    for col in sample_df.columns:\n",
    "        s = sample_df[col].dropna()\n",
    "        if s.empty:\n",
    "            dtype_map[col] = Text()\n",
    "            continue\n",
    "        if pd.api.types.is_integer_dtype(s):\n",
    "            try:\n",
    "                mx = int(s.max())\n",
    "                mn = int(s.min())\n",
    "                if mn < -2147483648 or mx > 2147483647:\n",
    "                    dtype_map[col] = BigInteger()\n",
    "                else:\n",
    "                    dtype_map[col] = Integer()\n",
    "            except Exception:\n",
    "                dtype_map[col] = BigInteger()\n",
    "        elif pd.api.types.is_float_dtype(s):\n",
    "            if engine_kind == \"mssql\":\n",
    "                dtype_map[col] = DECIMAL(38, 10)\n",
    "            else:\n",
    "                dtype_map[col] = Float()\n",
    "        elif pd.api.types.is_bool_dtype(s):\n",
    "            dtype_map[col] = Boolean()\n",
    "        elif pd.api.types.is_datetime64_any_dtype(s):\n",
    "            dtype_map[col] = DateTime()\n",
    "        else:\n",
    "            if engine_kind == \"mssql\":\n",
    "                dtype_map[col] = String(length=None)\n",
    "            else:\n",
    "                dtype_map[col] = Text()\n",
    "    return dtype_map\n",
    "\n",
    "def load_csv_to_sql(engine: Engine, table_name: str, csv_path: str, engine_kind: str, chunksize: int = 100_000):\n",
    "    it = pd.read_csv(csv_path, nrows=5000)\n",
    "    dt_cols = [c for c in it.columns if re.search(r\"(date|time|timestamp)$\", c, flags=re.I)]\n",
    "    if dt_cols:\n",
    "        it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
    "    dtype_map = infer_sqlalchemy_dtypes(it, engine_kind)\n",
    "\n",
    "    reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
    "    created = False\n",
    "    total_rows = 0\n",
    "    for i, chunk in enumerate(reader):\n",
    "        chunk.columns = [safe_name(c) for c in chunk.columns]\n",
    "        if not created:\n",
    "            chunk.to_sql(table_name, engine, if_exists=\"replace\", index=False, dtype=dtype_map, method=None)\n",
    "            created = True\n",
    "        else:\n",
    "            chunk.to_sql(table_name, engine, if_exists=\"append\", index=False, method=None)\n",
    "        total_rows += len(chunk)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Loaded {total_rows} rows into {table_name} ...\")\n",
    "    if not created:\n",
    "        df = pd.read_csv(csv_path, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
    "        df.columns = [safe_name(c) for c in df.columns]\n",
    "        df.to_sql(table_name, engine, if_exists=\"replace\", index=False, dtype=infer_sqlalchemy_dtypes(df, engine_kind))\n",
    "        total_rows = len(df)\n",
    "    return total_rows\n",
    "\n",
    "def classify_tables(table_names):\n",
    "    fact = None\n",
    "    dims = []\n",
    "    for t in table_names:\n",
    "        if re.search(r\"(^|_)fact($|_)\", t):\n",
    "            fact = t\n",
    "        else:\n",
    "            dims.append(t)\n",
    "    if fact is None and table_names:\n",
    "        fact = sorted(table_names)[0]\n",
    "    return fact, dims\n",
    "\n",
    "def inspect_schema(engine: Engine, schema=None):\n",
    "    insp = sa.inspect(engine)\n",
    "    tables = insp.get_table_names(schema=schema)\n",
    "    info = {}\n",
    "    for t in tables:\n",
    "        cols = insp.get_columns(t, schema=schema)\n",
    "        info[t] = {\n",
    "            \"columns\": cols,\n",
    "            \"pk\": [c[\"name\"] for c in cols if c.get(\"primary_key\", False)]\n",
    "        }\n",
    "    return info\n",
    "\n",
    "def guess_roles(engine: Engine, table: str):\n",
    "    insp = sa.inspect(engine)\n",
    "    cols = [c[\"name\"] for c in insp.get_columns(table)]\n",
    "    id_cols = [c for c in cols if c == \"id\" or c.endswith(\"_id\")]\n",
    "    dt_cols = []\n",
    "    for c in cols:\n",
    "        try:\n",
    "            typ = insp.get_columns(table, c)[0][\"type\"]\n",
    "        except Exception:\n",
    "            typ = None\n",
    "        if re.search(r\"(date|time|timestamp)$\", c):\n",
    "            dt_cols.append(c)\n",
    "    if not dt_cols:\n",
    "        dt_cols = [c for c in cols if re.search(r\"(date|time)\", c)]\n",
    "    measure_cols = []\n",
    "    for c in cols:\n",
    "        if c in id_cols:\n",
    "            continue\n",
    "        try:\n",
    "            typ = insp.get_columns(table, c)[0][\"type\"]\n",
    "            text_typ = str(typ).lower()\n",
    "            if any(k in text_typ for k in [\"int\", \"bigint\", \"float\", \"double\", \"numeric\", \"decimal\"]):\n",
    "                measure_cols.append(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "    id_cols = id_cols[:5]\n",
    "    dt_col = dt_cols[0] if dt_cols else None\n",
    "    measure_col = measure_cols[0] if measure_cols else None\n",
    "    return {\"id_cols\": id_cols, \"dt_col\": dt_col, \"measure_col\": measure_col, \"all_cols\": cols}\n",
    "\n",
    "def create_indexes_and_fks(engine: Engine, fact: str, dims: list, dialect: str):\n",
    "    insp = sa.inspect(engine)\n",
    "    with engine.begin() as conn:\n",
    "        for d in dims:\n",
    "            dcols = [c[\"name\"] for c in insp.get_columns(d)]\n",
    "            if \"id\" in dcols:\n",
    "                fk_col = f\"{d}_id\"\n",
    "                fcols = [c[\"name\"] for c in insp.get_columns(fact)]\n",
    "                if fk_col in fcols:\n",
    "                    idx_name = f\"ix_{fact}_{fk_col}\"\n",
    "                    try:\n",
    "                        conn.execute(text(f\"CREATE INDEX IF NOT EXISTS {idx_name} ON {fact} ({fk_col})\"))\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            conn.execute(text(f\"IF NOT EXISTS (SELECT name FROM sys.indexes WHERE name = '{idx_name}') CREATE INDEX {idx_name} ON {fact} ({fk_col})\"))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Index creation skipped for {fk_col}: {e}\")\n",
    "                    fk_name = f\"fk_{fact}_{fk_col}_{d}_id\"\n",
    "                    try:\n",
    "                        conn.execute(text(f\"ALTER TABLE {fact} ADD CONSTRAINT {fk_name} FOREIGN KEY ({fk_col}) REFERENCES {d}(id)\"))\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        roles = guess_roles(engine, fact)\n",
    "        if roles[\"dt_col\"]:\n",
    "            idxdt = f\"ix_{fact}_{roles['dt_col']}\"\n",
    "            try:\n",
    "                conn.execute(text(f\"CREATE INDEX IF NOT EXISTS {idxdt} ON {fact} ({roles['dt_col']})\"))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    conn.execute(text(f\"IF NOT EXISTS (SELECT name FROM sys.indexes WHERE name = '{idxdt}') CREATE INDEX {idxdt} ON {fact} ({roles['dt_col']})\"))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def monitor_docker_stats(container_name, stop_event, interval=1.0):\n",
    "    samples = []\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            out = run_cmd([\"docker\", \"stats\", container_name, \"--no-stream\", \"--format\", \"{{json .}}\"], check=False)\n",
    "            if out.strip():\n",
    "                try:\n",
    "                    rec = json.loads(out.strip())\n",
    "                    cpu = float(str(rec.get(\"CPUPerc\",\"0\")).strip(\"%\"))\n",
    "                    mem_perc = float(str(rec.get(\"MemPerc\",\"0\")).strip(\"%\"))\n",
    "                    mem_usage = rec.get(\"MemUsage\",\"0 / 0\")\n",
    "                    samples.append({\"ts\": time.time(), \"cpu_percent\": cpu, \"mem_percent\": mem_perc, \"mem_usage_raw\": mem_usage})\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(interval)\n",
    "    return samples\n",
    "\n",
    "def p95(values):\n",
    "    if not values:\n",
    "        return None\n",
    "    s = sorted(values)\n",
    "    k = int(math.ceil(0.95 * len(s))) - 1\n",
    "    return s[max(0, min(k, len(s)-1))]\n",
    "\n",
    "def summarise_stats(samples):\n",
    "    if not samples:\n",
    "        return {\"avg_cpu_percent\": None, \"avg_mem_percent\": None, \"p95_cpu_percent\": None, \"p95_mem_percent\": None}\n",
    "    cpus = [x[\"cpu_percent\"] for x in samples if x.get(\"cpu_percent\") is not None]\n",
    "    mems = [x[\"mem_percent\"] for x in samples if x.get(\"mem_percent\") is not None]\n",
    "    return {\n",
    "        \"avg_cpu_percent\": sum(cpus)/len(cpus) if cpus else None,\n",
    "        \"avg_mem_percent\": sum(mems)/len(mems) if mems else None,\n",
    "        \"p95_cpu_percent\": p95(cpus) if cpus else None,\n",
    "        \"p95_mem_percent\": p95(mems) if mems else None,\n",
    "        \"num_samples\": len(samples)\n",
    "    }\n",
    "\n",
    "class WorkloadRunner:\n",
    "    def __init__(self, engine: Engine, dialect: str, fact: str, dims: list, duration_sec: int = 60, concurrency: int = 8, read_ratio: float = 0.8):\n",
    "        self.engine = engine\n",
    "        self.dialect = dialect\n",
    "        self.fact = fact\n",
    "        self.dims = dims\n",
    "        self.duration_sec = duration_sec\n",
    "        self.concurrency = concurrency\n",
    "        self.read_ratio = read_ratio\n",
    "        self.roles = guess_roles(engine, fact)\n",
    "        self._stop_time = time.time() + duration_sec\n",
    "        self.latencies = []\n",
    "        self.errors = 0\n",
    "        self.completed = 0\n",
    "\n",
    "    def _sql_now(self):\n",
    "        return \"CURRENT_TIMESTAMP\" if self.dialect == \"postgresql\" else \"SYSDATETIME()\"\n",
    "\n",
    "    def _month_trunc(self, col):\n",
    "        if self.dialect == \"postgresql\":\n",
    "            return f\"date_trunc('month', {col})\"\n",
    "        else:\n",
    "            return f\"DATETRUNC(month, {col})\"\n",
    "\n",
    "    def _random_read_query(self):\n",
    "        m = self.roles[\"measure_col\"]\n",
    "        dt = self.roles[\"dt_col\"]\n",
    "        id_cols = self.roles[\"id_cols\"]\n",
    "        if m and dt:\n",
    "            return f\"SELECT {self._month_trunc(dt)} AS m, SUM({m}) AS s FROM {self.fact} GROUP BY {self._month_trunc(dt)} ORDER BY m DESC OFFSET 0 ROWS FETCH NEXT 50 ROWS ONLY\" if self.dialect == \"mssql\" else f\"SELECT {self._month_trunc(dt)} AS m, SUM({m}) AS s FROM {self.fact} GROUP BY {self._month_trunc(dt)} ORDER BY m DESC LIMIT 50\"\n",
    "        if id_cols:\n",
    "            fk = id_cols[0]\n",
    "            match_dim = None\n",
    "            for d in self.dims:\n",
    "                if fk == f\"{d}_id\":\n",
    "                    match_dim = d\n",
    "                    break\n",
    "            if match_dim:\n",
    "                return f\"SELECT d.id, COUNT(*) AS c FROM {self.fact} f JOIN {match_dim} d ON f.{fk} = d.id GROUP BY d.id ORDER BY c DESC\"\n",
    "        return f\"SELECT COUNT(*) FROM {self.fact}\"\n",
    "\n",
    "    def _random_write_query(self):\n",
    "        id_cols = self.roles[\"id_cols\"]\n",
    "        dt = self.roles[\"dt_col\"]\n",
    "        m = self.roles[\"measure_col\"]\n",
    "        cols = self.roles[\"all_cols\"]\n",
    "        if id_cols and m:\n",
    "            with self.engine.begin() as conn:\n",
    "                fk = id_cols[0]\n",
    "                try:\n",
    "                    keys = conn.execute(text(f\"SELECT {fk} FROM {self.fact} WHERE {fk} IS NOT NULL ORDER BY {self._dialect_random()} LIMIT 1000\" if self.dialect==\"postgresql\" else f\"SELECT TOP 1000 {fk} FROM {self.fact} WHERE {fk} IS NOT NULL ORDER BY NEWID()\")).fetchall()\n",
    "                    key = keys[random.randrange(len(keys))][0] if keys else None\n",
    "                except Exception:\n",
    "                    key = None\n",
    "            val_m = round(random.random() * 1000, 6)\n",
    "            cols_in = []\n",
    "            vals_in = []\n",
    "            if id_cols and key is not None:\n",
    "                cols_in.append(fk)\n",
    "                vals_in.append(str(key))\n",
    "            if dt:\n",
    "                cols_in.append(dt)\n",
    "                vals_in.append(self._now_literal())\n",
    "            if m:\n",
    "                cols_in.append(m)\n",
    "                vals_in.append(str(val_m))\n",
    "            if not cols_in:\n",
    "                cols_in = [m]\n",
    "                vals_in = [str(val_m)]\n",
    "            return f\"INSERT INTO {self.fact} ({', '.join(cols_in)}) VALUES ({', '.join(vals_in)})\"\n",
    "        target_col = None\n",
    "        with self.engine.begin() as conn:\n",
    "            for c in cols:\n",
    "                if c not in id_cols and c != dt:\n",
    "                    target_col = c\n",
    "                    break\n",
    "        if target_col:\n",
    "            return f\"UPDATE {self.fact} SET {target_col} = {target_col} WHERE 1=0\"\n",
    "        return f\"SELECT 1\"\n",
    "\n",
    "    def _now_literal(self):\n",
    "        return self._sql_now()\n",
    "\n",
    "    def _dialect_random(self):\n",
    "        return \"random()\" if self.dialect == \"postgresql\" else \"NEWID()\"\n",
    "\n",
    "    def _exec(self, sql: str):\n",
    "        t0 = time.perf_counter()\n",
    "        ok = True\n",
    "        err = \"\"\n",
    "        try:\n",
    "            with self.engine.begin() as conn:\n",
    "                conn.execute(text(sql))\n",
    "        except Exception as e:\n",
    "            ok = False\n",
    "            err = str(e)[:200]\n",
    "        dt = time.perf_counter() - t0\n",
    "        self.latencies.append({\"ts\": time.time(), \"sql\": sql, \"ok\": ok, \"latency_ms\": dt * 1000.0, \"dialect\": self.dialect})\n",
    "        if ok:\n",
    "            self.completed += 1\n",
    "        else:\n",
    "            self.errors += 1\n",
    "\n",
    "    def _worker(self):\n",
    "        rng = random.Random()\n",
    "        while time.time() < self._stop_time:\n",
    "            q = self._random_read_query() if rng.random() < self.read_ratio else self._random_write_query()\n",
    "            self._exec(q)\n",
    "\n",
    "    def run(self):\n",
    "        stop_event = threading.Event()\n",
    "        self._mon_samples = []\n",
    "        def mon_wrapper():\n",
    "            self._mon_samples.extend(monitor_docker_stats(self._container_name, stop_event, 1.0))\n",
    "        mon_thr = threading.Thread(target=mon_wrapper, daemon=True)\n",
    "        mon_thr.start()\n",
    "        t_start = time.time()\n",
    "        with ThreadPoolExecutor(max_workers=self.concurrency) as ex:\n",
    "            futs = [ex.submit(self._worker) for _ in range(self.concurrency)]\n",
    "            for f in as_completed(futs):\n",
    "                pass\n",
    "        stop_event.set()\n",
    "        mon_thr.join(timeout=2.0)\n",
    "        t_end = time.time()\n",
    "        self.elapsed = t_end - t_start\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def _container_name(self):\n",
    "        return \"pg_bench\" if self.dialect == \"postgresql\" else \"sqlserver_bench\"\n",
    "\n",
    "    def summary(self):\n",
    "        lats_ok = [x[\"latency_ms\"] for x in self.latencies if x[\"ok\"]]\n",
    "        return {\n",
    "            \"transactions\": self.completed,\n",
    "            \"errors\": self.errors,\n",
    "            \"elapsed_sec\": self.elapsed,\n",
    "            \"throughput_tps\": (self.completed / self.elapsed) if self.elapsed > 0 else None,\n",
    "            \"p95_latency_ms\": p95(lats_ok) if lats_ok else None,\n",
    "            \"cpu_mem_summary\": summarise_stats(self._mon_samples)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7339a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "CSV_DIR = r\"C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\"\n",
    "SA_PASSWORD = \"YourStrong!Passw0rd\"\n",
    "MSSQL_DB = \"Bench\"\n",
    "MSSQL_PORT = 11433\n",
    "MSSQL_CONTAINER = \"sqlserver_bench\"\n",
    "MSSQL_IMAGE = \"mcr.microsoft.com/mssql/server:2022-latest\"\n",
    "DATA_VOLUME = \"sqlserver_bench_data\"\n",
    "\n",
    "assert_docker()\n",
    "\n",
    "import pyodbc\n",
    "drivers = [d for d in pyodbc.drivers()]\n",
    "if not any(\"ODBC Driver 18 for SQL Server\" in d for d in drivers):\n",
    "    print(\"WARNING: 'ODBC Driver 18 for SQL Server' not detected on host. Install it if connection fails.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8533b21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV_DIR: C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\n",
      "Running: docker run -d --name sqlserver_bench -e ACCEPT_EULA=Y -e MSSQL_SA_PASSWORD=YourStrong!Passw0rd -p 11433:1433 -v sqlserver_bench_data:/var/opt/mssql -v C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV:/csv mcr.microsoft.com/mssql/server:2022-latest\n",
      "Port ready: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start SQL Server container\n",
    "mount_src = os.path.abspath(CSV_DIR)\n",
    "print(\"CSV_DIR:\", mount_src)\n",
    "existing = run_cmd([\"docker\", \"ps\", \"-a\", \"--format\", \"{{.Names}}\"])\n",
    "if MSSQL_CONTAINER in existing.splitlines():\n",
    "    state = run_cmd([\"docker\", \"inspect\", \"-f\", \"{{.State.Status}}\", MSSQL_CONTAINER])\n",
    "    if state != \"running\":\n",
    "        print(\"Removing existing container:\", MSSQL_CONTAINER)\n",
    "        run_cmd([\"docker\", \"rm\", \"-f\", MSSQL_CONTAINER], check=False)\n",
    "\n",
    "run_cmd([\"docker\", \"pull\", MSSQL_IMAGE])\n",
    "cmd = [\n",
    "    \"docker\", \"run\", \"-d\",\n",
    "    \"--name\", MSSQL_CONTAINER,\n",
    "    \"-e\", \"ACCEPT_EULA=Y\",\n",
    "    \"-e\", f\"MSSQL_SA_PASSWORD={SA_PASSWORD}\",\n",
    "    \"-p\", f\"{MSSQL_PORT}:1433\",\n",
    "    \"-v\", f\"{DATA_VOLUME}:/var/opt/mssql\",\n",
    "    \"-v\", f\"{mount_src}:/csv\",\n",
    "    MSSQL_IMAGE\n",
    "]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "run_cmd(cmd)\n",
    "\n",
    "def wait_port(host, port, timeout=120):\n",
    "    t0 = time.time()\n",
    "    while time.time() - t0 < timeout:\n",
    "        s = socket.socket()\n",
    "        s.settimeout(2.0)\n",
    "        try:\n",
    "            s.connect((host, port))\n",
    "            s.close()\n",
    "            return True\n",
    "        except Exception:\n",
    "            time.sleep(1.0)\n",
    "    return False\n",
    "\n",
    "ok = wait_port(\"127.0.0.1\", MSSQL_PORT, 120)\n",
    "print(\"Port ready:\", ok)\n",
    "time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55765e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: Bench\n",
      "Connected to: mssql+pyodbc://sa:***@127.0.0.1:11433/Bench?TrustServerCertificate=yes&driver=ODBC+Driver+18+for+SQL+Server\n"
     ]
    }
   ],
   "source": [
    "# Connect via SQLAlchemy + pyodbc and create database (autocommit-safe for CREATE DATABASE)\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "base_url = URL.create(\n",
    "    drivername=\"mssql+pyodbc\",\n",
    "    username=\"sa\",\n",
    "    password=SA_PASSWORD,\n",
    "    host=\"127.0.0.1\",\n",
    "    port=MSSQL_PORT,\n",
    "    database=\"master\",\n",
    "    query={\"driver\": \"ODBC Driver 18 for SQL Server\", \"TrustServerCertificate\": \"yes\"},\n",
    ")\n",
    "admin_engine = sa.create_engine(base_url, pool_pre_ping=True, future=True, fast_executemany=True)\n",
    "\n",
    "# Basic connectivity sanity check\n",
    "with admin_engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT 1\"))\n",
    "\n",
    "# Check if target DB exists (no transaction needed)\n",
    "with admin_engine.connect() as conn:\n",
    "    exists = conn.execute(text(\"SELECT DB_ID(:n)\"), {\"n\": MSSQL_DB}).scalar()\n",
    "\n",
    "# Create the database outside any transaction\n",
    "if exists is None:\n",
    "    with admin_engine.connect().execution_options(isolation_level=\"AUTOCOMMIT\") as conn:\n",
    "        conn.execute(text(f\"CREATE DATABASE [{MSSQL_DB}]\"))\n",
    "        print(\"Created database:\", MSSQL_DB)\n",
    "else:\n",
    "    print(\"Database exists:\", MSSQL_DB)\n",
    "\n",
    "# Connect to the target database\n",
    "bench_url = URL.create(\n",
    "    drivername=\"mssql+pyodbc\",\n",
    "    username=\"sa\",\n",
    "    password=SA_PASSWORD,\n",
    "    host=\"127.0.0.1\",\n",
    "    port=MSSQL_PORT,\n",
    "    database=MSSQL_DB,\n",
    "    query={\"driver\": \"ODBC Driver 18 for SQL Server\", \"TrustServerCertificate\": \"yes\"},\n",
    ")\n",
    "engine = sa.create_engine(bench_url, pool_pre_ping=True, future=True, fast_executemany=True)\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"SELECT DB_NAME()\"))\n",
    "print(\"Connected to:\", bench_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f280cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\campaign_desc.csv -> table campaign_desc\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\campaign_table.csv -> table campaign_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\causal_data.csv -> table causal_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000000 rows into causal_data ...\n",
      "Loaded 2000000 rows into causal_data ...\n",
      "Loaded 3000000 rows into causal_data ...\n",
      "Loaded 4000000 rows into causal_data ...\n",
      "Loaded 5000000 rows into causal_data ...\n",
      "Loaded 6000000 rows into causal_data ...\n",
      "Loaded 7000000 rows into causal_data ...\n",
      "Loaded 8000000 rows into causal_data ...\n",
      "Loaded 9000000 rows into causal_data ...\n",
      "Loaded 10000000 rows into causal_data ...\n",
      "Loaded 11000000 rows into causal_data ...\n",
      "Loaded 12000000 rows into causal_data ...\n",
      "Loaded 13000000 rows into causal_data ...\n",
      "Loaded 14000000 rows into causal_data ...\n",
      "Loaded 15000000 rows into causal_data ...\n",
      "Loaded 16000000 rows into causal_data ...\n",
      "Loaded 17000000 rows into causal_data ...\n",
      "Loaded 18000000 rows into causal_data ...\n",
      "Loaded 19000000 rows into causal_data ...\n",
      "Loaded 20000000 rows into causal_data ...\n",
      "Loaded 21000000 rows into causal_data ...\n",
      "Loaded 22000000 rows into causal_data ...\n",
      "Loaded 23000000 rows into causal_data ...\n",
      "Loaded 24000000 rows into causal_data ...\n",
      "Loaded 25000000 rows into causal_data ...\n",
      "Loaded 26000000 rows into causal_data ...\n",
      "Loaded 27000000 rows into causal_data ...\n",
      "Loaded 28000000 rows into causal_data ...\n",
      "Loaded 29000000 rows into causal_data ...\n",
      "Loaded 30000000 rows into causal_data ...\n",
      "Loaded 31000000 rows into causal_data ...\n",
      "Loaded 32000000 rows into causal_data ...\n",
      "Loaded 33000000 rows into causal_data ...\n",
      "Loaded 34000000 rows into causal_data ...\n",
      "Loaded 35000000 rows into causal_data ...\n",
      "Loaded 36000000 rows into causal_data ...\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\coupon.csv -> table coupon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\coupon_redempt.csv -> table coupon_redempt\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\hh_demographic.csv -> table hh_demographic\n",
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\product.csv -> table product\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\uhati\\Desktop\\Project phase 2\\CSV\\transaction_data.csv -> table transaction_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:119: FutureWarning: The 'keep_date_col' keyword in pd.read_csv is deprecated and will be removed in a future version. Explicitly remove unwanted columns after parsing instead.\n",
      "  it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:119: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:119: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  it = pd.read_csv(csv_path, nrows=5000, parse_dates=dt_cols, infer_datetime_format=True, dayfirst=False, keep_date_col=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:122: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  reader = pd.read_csv(csv_path, chunksize=chunksize, parse_dates=dt_cols if dt_cols else None, infer_datetime_format=True, encoding_errors=\"ignore\")\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000000 rows into transaction_data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 rows into transaction_data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n",
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\3836136477.py:125: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(reader):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tables: {'campaign_desc': 30, 'campaign_table': 7208, 'causal_data': 36786524, 'coupon': 124548, 'coupon_redempt': 2318, 'hh_demographic': 801, 'product': 92353, 'transaction_data': 2595732}\n",
      "Load time seconds: 486.21\n",
      "Fact table: campaign_desc\n",
      "Dim tables: ['campaign_desc', 'campaign_table', 'causal_data', 'coupon', 'coupon_redempt', 'hh_demographic', 'product', 'transaction_data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load all CSVs into tables\n",
    "csvs = list_csvs(CSV_DIR)\n",
    "table_map = {}\n",
    "rows_loaded = {}\n",
    "t0 = time.time()\n",
    "for f in csvs:\n",
    "    name = safe_name(Path(f).stem)\n",
    "    table_map[name] = f\n",
    "    print(f\"Loading {f} -> table {name}\")\n",
    "    n = load_csv_to_sql(engine, name, f, \"mssql\", chunksize=100_000)\n",
    "    rows_loaded[name] = n\n",
    "t_load = time.time() - t0\n",
    "print(\"Loaded tables:\", rows_loaded)\n",
    "print(f\"Load time seconds: {t_load:.2f}\")\n",
    "\n",
    "fact, dims = classify_tables(list(table_map.keys()))\n",
    "print(\"Fact table:\", fact)\n",
    "print(\"Dim tables:\", dims)\n",
    "\n",
    "create_indexes_and_fks(engine, fact, dims, \"mssql\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e6e99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: C:\\Users\\uhati\\Desktop\\Project phase 2\\results\n",
      "Schema roles before: {'id_cols': [], 'dt_col': None, 'measure_col': None, 'all_cols': ['description', 'campaign', 'start_day', 'end_day']}\n",
      "Schema roles now: {'id_cols': [], 'dt_col': None, 'measure_col': None, 'all_cols': ['description', 'campaign', 'start_day', 'end_day']}\n",
      "Benchmark summary: {\n",
      "  \"transactions\": 155971,\n",
      "  \"errors\": 0,\n",
      "  \"elapsed_sec\": 59.99884390830994,\n",
      "  \"throughput_tps\": 2599.566755625399,\n",
      "  \"p95_latency_ms\": 3.312099986942485,\n",
      "  \"cpu_mem_summary\": {\n",
      "    \"avg_cpu_percent\": 251.5090476190476,\n",
      "    \"avg_mem_percent\": 45.79857142857143,\n",
      "    \"p95_cpu_percent\": 256.1,\n",
      "    \"p95_mem_percent\": 45.81,\n",
      "    \"num_samples\": 21\n",
      "  }\n",
      "}\n",
      "Latency details: C:\\Users\\uhati\\Desktop\\Project phase 2\\results\\mssql_latency_detail.csv\n",
      "Saved results: C:\\Users\\uhati\\Desktop\\Project phase 2\\results\\mssql_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhati\\AppData\\Local\\Temp\\ipykernel_19200\\4194486713.py:110: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    }
   ],
   "source": [
    "# Define and run the workload (Windows-safe outputs + optional date promotion)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Where to save results on your machine\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\uhati\\Desktop\\Project phase 2\\results\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MSSQL_LAT_PATH = OUTPUT_DIR / \"mssql_latency_detail.csv\"\n",
    "MSSQL_JSON_PATH = OUTPUT_DIR / \"mssql_results.json\"\n",
    "print(\"Results will be saved to:\", OUTPUT_DIR)\n",
    "\n",
    "# Inspect schema and roles\n",
    "info = inspect_schema(engine)\n",
    "roles = guess_roles(engine, fact)\n",
    "print(\"Schema roles before:\", roles)\n",
    "\n",
    "# Optional: if no timestamp detected, attempt a safe promotion of common date-like text columns\n",
    "if roles[\"dt_col\"] is None:\n",
    "    with engine.begin() as conn:\n",
    "        insp = sa.inspect(engine)\n",
    "        for t in insp.get_table_names():\n",
    "            cols = {c[\"name\"]: c for c in insp.get_columns(t)}\n",
    "            candidates = [\n",
    "                \"start_day\",\"end_day\",\"start_date\",\"end_date\",\n",
    "                \"created_at\",\"updated_at\",\"timestamp\",\"event_time\",\"date\",\"datetime\"\n",
    "            ]\n",
    "            for col in candidates:\n",
    "                if col in cols:\n",
    "                    typ_str = str(cols[col][\"type\"]).lower()\n",
    "                    if any(s in typ_str for s in [\"char\",\"text\",\"nchar\",\"nvarchar\",\"varchar\"]):\n",
    "                        try:\n",
    "                            conn.execute(text(f\"ALTER TABLE [{t}] ADD [{col}_tmp] datetime2 NULL\"))\n",
    "                            conn.execute(text(f\"UPDATE [{t}] SET [{col}_tmp] = TRY_CONVERT(datetime2, NULLIF([{col}], ''))\"))\n",
    "                            conn.execute(text(f\"ALTER TABLE [{t}] DROP COLUMN [{col}]\"))\n",
    "                            conn.execute(text(f\"EXEC sp_rename 'dbo.{t}.{col}_tmp', '{col}', 'COLUMN'\"))\n",
    "                            print(f\"Converted {t}.{col} to datetime2\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Skipped {t}.{col}: {e}\")\n",
    "    # Refresh roles after attempted promotion\n",
    "    roles = guess_roles(engine, fact)\n",
    "\n",
    "print(\"Schema roles now:\", roles)\n",
    "\n",
    "# Workload configuration\n",
    "DURATION_SEC = 60\n",
    "CONCURRENCY = 8\n",
    "READ_RATIO = 0.8\n",
    "\n",
    "# Run the benchmark\n",
    "runner = WorkloadRunner(engine, \"mssql\", fact, dims,\n",
    "                        duration_sec=DURATION_SEC,\n",
    "                        concurrency=CONCURRENCY,\n",
    "                        read_ratio=READ_RATIO)\n",
    "runner.run()\n",
    "summary = runner.summary()\n",
    "print(\"Benchmark summary:\", json.dumps(summary, indent=2))\n",
    "\n",
    "# Save latency details\n",
    "lat_df = pd.DataFrame(runner.latencies)\n",
    "lat_df.to_csv(MSSQL_LAT_PATH, index=False)\n",
    "print(\"Latency details:\", MSSQL_LAT_PATH)\n",
    "\n",
    "# Storage footprint\n",
    "with engine.begin() as conn:\n",
    "    db_bytes = conn.execute(\n",
    "        text(\"SELECT SUM(CAST(size AS bigint)) * 8 * 1024 FROM sys.master_files WHERE database_id = DB_ID(:d)\"),\n",
    "        {\"d\": MSSQL_DB}\n",
    "    ).scalar()\n",
    "    per_table = conn.execute(text(\"\"\"\n",
    "        SELECT\n",
    "            t.name AS table_name,\n",
    "            SUM(CAST(a.total_pages AS bigint)) * 8 * 1024 AS bytes\n",
    "        FROM sys.tables t\n",
    "        JOIN sys.indexes i\n",
    "          ON t.object_id = i.object_id\n",
    "        JOIN sys.partitions p\n",
    "          ON i.object_id = p.object_id AND i.index_id = p.index_id\n",
    "        JOIN sys.allocation_units a\n",
    "          ON p.partition_id = a.container_id\n",
    "        GROUP BY t.name\n",
    "        ORDER BY bytes DESC\n",
    "    \"\"\")).mappings().all()\n",
    "\n",
    "store = {\n",
    "    \"database_bytes\": int(db_bytes) if db_bytes is not None else None,\n",
    "    \"tables\": [{\"table\": r[\"table_name\"], \"bytes\": int(r[\"bytes\"])} for r in per_table]\n",
    "}\n",
    "\n",
    "# Engine version\n",
    "try:\n",
    "    ver = run_cmd([\n",
    "        \"docker\", \"exec\", MSSQL_CONTAINER,\n",
    "        \"/opt/mssql-tools18/bin/sqlcmd\",\n",
    "        \"-S\", \"localhost\", \"-U\", \"sa\", \"-P\", SA_PASSWORD, \"-C\", \"-Q\", \"SELECT @@VERSION\"\n",
    "    ], check=False)\n",
    "except Exception:\n",
    "    ver = \"\"\n",
    "\n",
    "# Results package\n",
    "results = {\n",
    "    \"engine\": \"sqlserver\",\n",
    "    \"version\": ver.strip(),\n",
    "    \"rows_loaded\": rows_loaded,\n",
    "    \"load_time_sec\": t_load,\n",
    "    \"workload\": {\"duration_sec\": DURATION_SEC, \"concurrency\": CONCURRENCY, \"read_ratio\": READ_RATIO},\n",
    "    \"bench_summary\": summary,\n",
    "    \"storage\": store,\n",
    "    \"admin_time_sec\": t_load,\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "}\n",
    "\n",
    "with open(MSSQL_JSON_PATH, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Saved results:\", MSSQL_JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3da8de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postgres detected. Running parity check on a set of read queries.\n",
      "Parity summary: [\n",
      "  {\n",
      "    \"sqlserver_rows\": 1,\n",
      "    \"postgres_rows\": 1,\n",
      "    \"shape_equal\": true,\n",
      "    \"checksum_equal\": true\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional parity check: if Postgres container is running, run the same read queries on both and compare aggregates\n",
    "try:\n",
    "    pg_state = run_cmd([\"docker\", \"inspect\", \"-f\", \"{{.State.Status}}\", \"pg_bench\"], check=False)\n",
    "    pg_available = \"running\" in pg_state\n",
    "except Exception:\n",
    "    pg_available = False\n",
    "\n",
    "if not pg_available:\n",
    "    print(\"Postgres container not detected in running state. Skipping parity check.\")\n",
    "else:\n",
    "    print(\"Postgres detected. Running parity check on a set of read queries.\")\n",
    "    from sqlalchemy.engine.url import URL\n",
    "    pg_url = URL.create(\n",
    "        drivername=\"postgresql+psycopg2\",\n",
    "        username=\"postgres\",\n",
    "        password=\"postgres\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=55432,\n",
    "        database=\"bench\",\n",
    "    )\n",
    "    pg_engine = sa.create_engine(pg_url, pool_pre_ping=True, future=True)\n",
    "\n",
    "    roles = guess_roles(engine, fact)\n",
    "    read_sqls_mssql = []\n",
    "    read_sqls_pg = []\n",
    "    read_sqls_mssql.append(f\"SELECT COUNT(*) AS c FROM {fact}\")\n",
    "    read_sqls_pg.append(f\"SELECT COUNT(*) AS c FROM {fact}\")\n",
    "    if roles[\"dt_col\"] and roles[\"measure_col\"]:\n",
    "        read_sqls_mssql.append(f\"SELECT TOP 100 DATETRUNC(month, {roles['dt_col']}) AS m, SUM({roles['measure_col']}) AS s FROM {fact} GROUP BY DATETRUNC(month, {roles['dt_col']}) ORDER BY m\")\n",
    "        read_sqls_pg.append(f\"SELECT date_trunc('month', {roles['dt_col']}) AS m, SUM({roles['measure_col']}) AS s FROM {fact} GROUP BY date_trunc('month', {roles['dt_col']}) ORDER BY m\")\n",
    "\n",
    "    def checksum_df(df):\n",
    "        if df is None or df.empty:\n",
    "            return 0\n",
    "        return int(pd.util.hash_pandas_object(df, index=False).sum())\n",
    "\n",
    "    parity = []\n",
    "    with engine.begin() as mconn, pg_engine.begin() as pconn:\n",
    "        for ms_sql, pg_sql in zip(read_sqls_mssql, read_sqls_pg):\n",
    "            ms_df = pd.read_sql(ms_sql, mconn)\n",
    "            pg_df = pd.read_sql(pg_sql, pconn)\n",
    "            same_shape = ms_df.shape == pg_df.shape\n",
    "            same_checksum = checksum_df(ms_df) == checksum_df(pg_df)\n",
    "            parity.append({\"sqlserver_rows\": int(ms_df.shape[0]), \"postgres_rows\": int(pg_df.shape[0]), \"shape_equal\": bool(same_shape), \"checksum_equal\": bool(same_checksum)})\n",
    "    print(\"Parity summary:\", json.dumps(parity, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20d4bc-e6c3-49d9-8eba-0d76189a00ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
