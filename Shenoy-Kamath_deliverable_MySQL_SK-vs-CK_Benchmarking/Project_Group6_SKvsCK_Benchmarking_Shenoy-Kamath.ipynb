{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af1eb10",
   "metadata": {},
   "source": [
    "#### Project_Week10_Shenoy-Kamath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5ef5a",
   "metadata": {},
   "source": [
    "# MSDS420 - Project - Group 6 - Retail Customer Behavior For Growth - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e966a",
   "metadata": {},
   "source": [
    "### Approach: Docker with MySQL  - Surrogate Key Vs Composite Key tables - Benchmarking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44434aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pymysql pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bedbc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql, time, pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39840ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from .env -> {'host': '127.0.0.1', 'port': '3307', 'db': 'retail_db', 'user': 'root'}\n",
      "Socket check OK -> can reach 127.0.0.1:3307\n",
      "Connecting: mysql+pymysql://root:******@127.0.0.1:3307/retail_db\n",
      "  mysql_version current_db\n",
      "0        8.0.43  retail_db\n"
     ]
    }
   ],
   "source": [
    "# --- clean connect to MySQL on 127.0.0.1:3307 using .env ---\n",
    "\n",
    "# 1) Load env (override any cached values)\n",
    "from dotenv import load_dotenv\n",
    "import os, socket\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "host = os.getenv(\"MYSQL_HOST\")\n",
    "port = os.getenv(\"MYSQL_PORT\")\n",
    "user = os.getenv(\"MYSQL_USER\")\n",
    "pwd  = os.getenv(\"MYSQL_PASSWORD\")\n",
    "db   = os.getenv(\"MYSQL_DB\")\n",
    "\n",
    "print(\"Loaded from .env ->\", {\"host\": host, \"port\": port, \"db\": db, \"user\": user})\n",
    "\n",
    "# 2) Assert the values we NEED\n",
    "if host != \"127.0.0.1\":\n",
    "    raise ValueError(f\"MYSQL_HOST must be 127.0.0.1, got {host!r}\")\n",
    "if str(port) != \"3307\":\n",
    "    raise ValueError(f\"MYSQL_PORT must be 3307 (per docker ps), got {port!r}\")\n",
    "if not all([user, pwd, db]):\n",
    "    raise ValueError(\"Missing MYSQL_USER / MYSQL_PASSWORD / MYSQL_DB\")\n",
    "\n",
    "# 3) Prove the TCP port is listening\n",
    "sock = socket.create_connection((host, int(port)), timeout=5)\n",
    "sock.close()\n",
    "print(\"Socket check OK -> can reach\", f\"{host}:{port}\")\n",
    "\n",
    "# 4) Build engine with these exact values (no reuse)\n",
    "url = f\"mysql+pymysql://{user}:{pwd}@{host}:{port}/{db}\"\n",
    "print(\"Connecting:\", url.replace(pwd, \"******\"))\n",
    "engine = create_engine(url, pool_pre_ping=True, connect_args={\"connect_timeout\": 5})\n",
    "\n",
    "# 5) Smoke test\n",
    "def q(sql, **params):\n",
    "    return pd.read_sql_query(text(sql), engine, params=params)\n",
    "\n",
    "print(q(\"SELECT VERSION() AS mysql_version, DATABASE() AS current_db;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6823a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYSQL_USER: root\n",
      "MYSQL_HOST: 127.0.0.1\n",
      "MYSQL_PORT: 3307\n",
      "MYSQL_DB: retail_db\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env from current folder\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# Print values to confirm (masking password)\n",
    "print(\"MYSQL_USER:\", os.getenv(\"MYSQL_USER\"))\n",
    "print(\"MYSQL_HOST:\", os.getenv(\"MYSQL_HOST\"))\n",
    "print(\"MYSQL_PORT:\", os.getenv(\"MYSQL_PORT\"))\n",
    "print(\"MYSQL_DB:\", os.getenv(\"MYSQL_DB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d971a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render high resolution plots\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0eaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to take an SQL string and optional parameters\n",
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "\n",
    "# Simple helper: run SQL and return a pandas DataFrame\n",
    "def q(sql, **params):\n",
    "    return pd.read_sql_query(text(sql), engine, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86e9593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>retail_db</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          db\n",
       "0  retail_db"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smoke test\n",
    "q(\"SELECT DATABASE() AS db;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ec45f",
   "metadata": {},
   "source": [
    "# PART 1 - Performance for Assignment specific queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ffa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper snippets (inline so no session vars needed)\n",
    "STORE   = \"(SELECT store_sk FROM bench_lines_data LIMIT 1)\"\n",
    "DAY_LO  = \"(SELECT MIN(`day`) FROM bench_lines_data)\"\n",
    "DAY_HI  = f\"({DAY_LO} + 30)\"  # ~30-day window\n",
    "HH      = \"(SELECT household_sk FROM bench_lines_data LIMIT 1)\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d02d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark window: store_sk=140, day 6..36\n",
      "  assignment_task design      ms  rowcount\n",
      "  campaign_impact     ck 6744.19         2\n",
      "  campaign_impact     sk 6119.41         2\n",
      "   demo_influence     ck  563.75        10\n",
      "   demo_influence     sk  488.05        10\n",
      "growth_categories     ck 1561.09        10\n",
      "growth_categories     sk 1247.66        10\n",
      "     spend_trends     ck    5.47       177\n",
      "     spend_trends     sk    6.63       177\n",
      "\n",
      "Results saved to benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- connection ----\n",
    "CONN = dict(host=\"127.0.0.1\", port=3307, user=\"bench\", password=\"benchpw\", database=\"retail_db\")\n",
    "\n",
    "def run(sql):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            t0 = time.perf_counter()\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "            ms = (time.perf_counter() - t0) * 1000\n",
    "            return rows, ms\n",
    "\n",
    "def choose_store_and_window():\n",
    "    \"\"\"Pick a busy store and a 30-day window that actually has rows.\"\"\"\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # 1) pick the busiest store in CK (SK mirrors data)\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT store_sk, MIN(`day`) AS min_day, MAX(`day`) AS max_day, COUNT(*) AS cnt\n",
    "                FROM bench_lines_ck\n",
    "                GROUP BY store_sk\n",
    "                ORDER BY cnt DESC\n",
    "                LIMIT 1\n",
    "            \"\"\")\n",
    "            rec = cur.fetchone()\n",
    "            store = int(rec[\"store_sk\"])\n",
    "            min_day, max_day = int(rec[\"min_day\"]), int(rec[\"max_day\"])\n",
    "\n",
    "            # first attempt: earliest 31-day window\n",
    "            day_lo = min_day\n",
    "            day_hi = min(min_day + 30, max_day)\n",
    "\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT COUNT(*) AS n\n",
    "                FROM bench_lines_ck\n",
    "                WHERE store_sk=%s AND `day` BETWEEN %s AND %s\n",
    "            \"\"\", (store, day_lo, day_hi))\n",
    "            n = cur.fetchone()[\"n\"]\n",
    "\n",
    "            # fallback: center on the densest single day, widen ±7\n",
    "            if n == 0:\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT `day`, COUNT(*) AS c\n",
    "                    FROM bench_lines_ck\n",
    "                    WHERE store_sk=%s\n",
    "                    GROUP BY `day`\n",
    "                    ORDER BY c DESC\n",
    "                    LIMIT 1\n",
    "                \"\"\", (store,))\n",
    "                best_day = int(cur.fetchone()[\"day\"])\n",
    "                day_lo = max(min_day, best_day - 7)\n",
    "                day_hi = min(max_day, best_day + 7)\n",
    "\n",
    "            return store, day_lo, day_hi\n",
    "\n",
    "# ---- choose constants for this run ----\n",
    "STORE, DAY_LO, DAY_HI = choose_store_and_window()\n",
    "print(f\"Benchmark window: store_sk={STORE}, day {DAY_LO}..{DAY_HI}\")\n",
    "\n",
    "# ---- build queries USING the constants above ----\n",
    "QUERIES = {\n",
    "    # 1) Track customer spending trends (by product within store & 30-day window)\n",
    "    \"spend_trends\": {\n",
    "        \"ck\": f\"\"\"\n",
    "            SELECT product_sk, SUM(sales) AS revenue\n",
    "            FROM bench_lines_ck\n",
    "            WHERE store_sk = {STORE} AND `day` BETWEEN {DAY_LO} AND {DAY_HI}\n",
    "            GROUP BY product_sk\n",
    "        \"\"\",\n",
    "        \"sk\": f\"\"\"\n",
    "            SELECT product_sk, SUM(sales) AS revenue\n",
    "            FROM bench_lines_sk\n",
    "            WHERE store_sk = {STORE} AND `day` BETWEEN {DAY_LO} AND {DAY_HI}\n",
    "            GROUP BY product_sk\n",
    "        \"\"\"\n",
    "    },\n",
    "\n",
    "    # 2) Evaluate demographic influences on customer spend\n",
    "    \"demo_influence\": {\n",
    "        \"ck\": f\"\"\"\n",
    "            SELECT h.income_desc, h.age_desc, SUM(b.sales) AS revenue\n",
    "            FROM bench_lines_ck b\n",
    "            JOIN household_dim h ON h.household_sk = b.household_sk\n",
    "            WHERE b.`day` BETWEEN {DAY_LO} AND {DAY_HI}\n",
    "            GROUP BY h.income_desc, h.age_desc\n",
    "            ORDER BY revenue DESC\n",
    "            LIMIT 10\n",
    "        \"\"\",\n",
    "        \"sk\": f\"\"\"\n",
    "            SELECT h.income_desc, h.age_desc, SUM(b.sales) AS revenue\n",
    "            FROM bench_lines_sk b\n",
    "            JOIN household_dim h ON h.household_sk = b.household_sk\n",
    "            WHERE b.`day` BETWEEN {DAY_LO} AND {DAY_HI}\n",
    "            GROUP BY h.income_desc, h.age_desc\n",
    "            ORDER BY revenue DESC\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "    },\n",
    "\n",
    "    # 3) Identify growth categories (current 30d vs previous 28d) using product_dim.commodity_desc\n",
    "    \"growth_categories\": {\n",
    "        \"ck\": f\"\"\"\n",
    "            WITH cur AS (\n",
    "              SELECT p.commodity_desc AS category, SUM(b.sales) AS rev\n",
    "              FROM bench_lines_ck b\n",
    "              JOIN product_dim p ON p.product_sk = b.product_sk\n",
    "              WHERE b.`day` BETWEEN {DAY_LO} AND {DAY_HI}\n",
    "              GROUP BY p.commodity_desc\n",
    "            ),\n",
    "            prev AS (\n",
    "              SELECT p.commodity_desc AS category, SUM(b.sales) AS rev\n",
    "              FROM bench_lines_ck b\n",
    "              JOIN product_dim p ON p.product_sk = b.product_sk\n",
    "              WHERE b.`day` BETWEEN ({DAY_LO} - 28) AND ({DAY_LO} - 1)\n",
    "              GROUP BY p.commodity_desc\n",
    "            )\n",
    "            SELECT c.category,\n",
    "                   c.rev AS rev_cur,\n",
    "                   COALESCE(p.rev,0) AS rev_prev,\n",
    "                   CASE WHEN COALESCE(p.rev,0)=0 THEN NULL\n",
    "                        ELSE (c.rev - p.rev)/p.rev END AS growth_ratio\n",
    "            FROM cur c\n",
    "            LEFT JOIN prev p USING (category)\n",
    "            ORDER BY growth_ratio DESC\n",
    "            LIMIT 10\n",
    "        \"\"\",\n",
    "        \"sk\": f\"\"\"\n",
    "            WITH cur AS (\n",
    "              SELECT p.commodity_desc AS category, SUM(b.sales) AS rev\n",
    "              FROM bench_lines_sk b\n",
    "              JOIN product_dim p ON p.product_sk = b.product_sk\n",
    "              WHERE b.`day` BETWEEN {DAY_LO} AND {DAY_HI}\n",
    "              GROUP BY p.commodity_desc\n",
    "            ),\n",
    "            prev AS (\n",
    "              SELECT p.commodity_desc AS category, SUM(b.sales) AS rev\n",
    "              FROM bench_lines_sk b\n",
    "              JOIN product_dim p ON p.product_sk = b.product_sk\n",
    "              WHERE b.`day` BETWEEN ({DAY_LO} - 28) AND ({DAY_LO} - 1)\n",
    "              GROUP BY p.commodity_desc\n",
    "            )\n",
    "            SELECT c.category,\n",
    "                   c.rev AS rev_cur,\n",
    "                   COALESCE(p.rev,0) AS rev_prev,\n",
    "                   CASE WHEN COALESCE(p.rev,0)=0 THEN NULL\n",
    "                        ELSE (c.rev - p.rev)/p.rev END AS growth_ratio\n",
    "            FROM cur c\n",
    "            LEFT JOIN prev p USING (category)\n",
    "            ORDER BY growth_ratio DESC\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "    },\n",
    "\n",
    "    # 4) Measure the impact of marketing campaigns\n",
    "    \"campaign_impact\": {\n",
    "        \"ck\": \"\"\"\n",
    "            WITH redeemed AS (\n",
    "              SELECT DISTINCT tf.product_sk, tf.store_sk, tf.`day`\n",
    "              FROM coupon_redemption_fact crf\n",
    "              JOIN bench_lines_ck tf\n",
    "                ON tf.household_sk = crf.household_sk\n",
    "               AND tf.product_sk   = crf.product_sk\n",
    "               AND tf.`day`        = crf.`day`\n",
    "            )\n",
    "            SELECT 'redeemed' AS flag, SUM(b.sales) AS revenue, SUM(b.qty) AS units\n",
    "            FROM bench_lines_ck b\n",
    "            JOIN redeemed r USING (product_sk, store_sk, `day`)\n",
    "            UNION ALL\n",
    "            SELECT 'not_redeemed', SUM(b.sales), SUM(b.qty)\n",
    "            FROM bench_lines_ck b\n",
    "            LEFT JOIN redeemed r USING (product_sk, store_sk, `day`)\n",
    "            WHERE r.product_sk IS NULL\n",
    "        \"\"\",\n",
    "        \"sk\": \"\"\"\n",
    "            WITH redeemed AS (\n",
    "              SELECT DISTINCT tf.product_sk, tf.store_sk, tf.`day`\n",
    "              FROM coupon_redemption_fact crf\n",
    "              JOIN bench_lines_sk tf\n",
    "                ON tf.household_sk = crf.household_sk\n",
    "               AND tf.product_sk   = crf.product_sk\n",
    "               AND tf.`day`        = crf.`day`\n",
    "            )\n",
    "            SELECT 'redeemed' AS flag, SUM(b.sales) AS revenue, SUM(b.qty) AS units\n",
    "            FROM bench_lines_sk b\n",
    "            JOIN redeemed r USING (product_sk, store_sk, `day`)\n",
    "            UNION ALL\n",
    "            SELECT 'not_redeemed', SUM(b.sales), SUM(b.qty)\n",
    "            FROM bench_lines_sk b\n",
    "            LEFT JOIN redeemed r USING (product_sk, store_sk, `day`)\n",
    "            WHERE r.product_sk IS NULL\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    for task, pair in QUERIES.items():\n",
    "        for design in (\"ck\", \"sk\"):\n",
    "            rows, ms = run(pair[design])\n",
    "            results.append({\n",
    "                \"assignment_task\": task,\n",
    "                \"design\": design,\n",
    "                \"ms\": round(ms, 2),\n",
    "                \"rowcount\": len(rows)\n",
    "            })\n",
    "    df = pd.DataFrame(results).sort_values([\"assignment_task\", \"design\"])\n",
    "    print(df.to_string(index=False))\n",
    "    df.to_csv(\"benchmark_results.csv\", index=False)\n",
    "    print(\"\\nResults saved to benchmark_results.csv\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb59a0",
   "metadata": {},
   "source": [
    "## Assignment Benchmarks — Store 140, Days 6..36\n",
    "\n",
    "### Results (ms)\n",
    "**Benchmark window:** `store_sk = 140`, `days 6..36`\n",
    "\n",
    "| assignment_task   | design |    ms   | rowcount |\n",
    "|-------------------|:------:|--------:|---------:|\n",
    "| campaign_impact   |  ck    | 6744.19 |        2 |\n",
    "| campaign_impact   |  sk    | 6119.41 |        2 |\n",
    "| demo_influence    |  ck    |  563.75 |       10 |\n",
    "| demo_influence    |  sk    |  488.05 |       10 |\n",
    "| growth_categories |  ck    | 1561.09 |       10 |\n",
    "| growth_categories |  sk    | 1247.66 |       10 |\n",
    "| spend_trends      |  ck    |    5.47 |      177 |\n",
    "| spend_trends      |  sk    |    6.63 |      177 |\n",
    "\n",
    "\n",
    "### Quick takeaways\n",
    "- **SK is faster on 3 of 4 tasks; CK wins `spend_trends` in this run.**  \n",
    "  - *spend_trends:* **CK ≈ 17% faster** (5.47 vs 6.63 ms) — CK’s clustering on `(store, day, product, …)` gives great locality.  \n",
    "  - *demo_influence:* **SK ≈ 13% faster** (488.05 vs 563.75 ms) — both scan/aggregate; SK edges out.  \n",
    "  - *growth_categories:* **SK ≈ 20% faster** (1247.66 vs 1561.09 ms) — join + group; SK benefits from secondary index paths.  \n",
    "  - *campaign_impact:* **SK ≈ 9% faster** (6119.41 vs 6744.19 ms) — heavy join/aggregate; differences are modest.\n",
    "\n",
    "### Interpretation\n",
    "- For **aligned access (store + day)**, both designs are efficient; in this sample **CK outperformed SK** on `spend_trends` due to clustered PK order.\n",
    "- For **join-heavy aggregations** (demo/growth/campaign), the fact-table scan + grouping dominates, so differences remain moderate; **SK often has a slight edge** thanks to targeted secondary indexes.\n",
    "- Remember the microbenchmarks:\n",
    "  - **Household-centric lookups:** **SK wins big** via `idx_household` (CK must scan).\n",
    "  - **Insert/maintenance & storage:** **CK wins** — smaller tables, fewer/lighter indexes → faster bulk inserts and table maintenance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468942b",
   "metadata": {},
   "source": [
    "# Part 2 - Comparsion between Surrogate key and Composite key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7445e5",
   "metadata": {},
   "source": [
    "## 1) Table & Index size (space overhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc2be6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table sizes (MB):\n",
      "    TABLE_NAME data_mb index_mb total_mb\n",
      "bench_lines_ck   98.69     0.00    98.69\n",
      "bench_lines_sk  109.66    75.17   184.83\n",
      "\n",
      "Index details:\n",
      "    TABLE_NAME         INDEX_NAME  NON_UNIQUE  SEQ_IN_INDEX  COLUMN_NAME SUB_PART INDEX_TYPE  CARDINALITY\n",
      "bench_lines_ck            PRIMARY           0             1     store_sk     None      BTREE          407\n",
      "bench_lines_ck            PRIMARY           0             2          day     None      BTREE        52770\n",
      "bench_lines_ck            PRIMARY           0             3   product_sk     None      BTREE      1410659\n",
      "bench_lines_ck            PRIMARY           0             4 household_sk     None      BTREE      1417842\n",
      "bench_lines_sk      idx_household           1             1 household_sk     None      BTREE          796\n",
      "bench_lines_sk idx_store_day_prod           1             1     store_sk     None      BTREE          393\n",
      "bench_lines_sk idx_store_day_prod           1             2          day     None      BTREE        53800\n",
      "bench_lines_sk idx_store_day_prod           1             3   product_sk     None      BTREE      1411597\n",
      "bench_lines_sk            PRIMARY           0             1           id     None      BTREE      1417143\n",
      "\n",
      " Saved: table_sizes.csv, index_details.csv\n"
     ]
    }
   ],
   "source": [
    "# Table + index bytes\n",
    "\n",
    "def run_sql(query):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            return cur.fetchall()\n",
    "\n",
    "# ---- 1) Table + index bytes (MB) ----\n",
    "q_table_sizes = \"\"\"\n",
    "SELECT table_name, \n",
    "       ROUND(data_length/1024/1024,2)  AS data_mb,\n",
    "       ROUND(index_length/1024/1024,2) AS index_mb,\n",
    "       ROUND((data_length+index_length)/1024/1024,2) AS total_mb\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema='retail_db'\n",
    "  AND table_name IN ('bench_lines_ck','bench_lines_sk')\n",
    "ORDER BY table_name;\n",
    "\"\"\"\n",
    "\n",
    "table_sizes_df = pd.DataFrame(run_sql(q_table_sizes))\n",
    "print(\"Table sizes (MB):\")\n",
    "print(table_sizes_df.to_string(index=False))\n",
    "table_sizes_df.to_csv(\"table_sizes.csv\", index=False)\n",
    "\n",
    "# ---- 2) Per-index details \n",
    "q_index_details = \"\"\"\n",
    "SELECT \n",
    "    table_name, \n",
    "    index_name, \n",
    "    NON_UNIQUE, \n",
    "    SEQ_IN_INDEX, \n",
    "    COLUMN_NAME, \n",
    "    SUB_PART, \n",
    "    INDEX_TYPE,\n",
    "    CARDINALITY\n",
    "FROM information_schema.STATISTICS\n",
    "WHERE table_schema='retail_db'\n",
    "  AND table_name IN ('bench_lines_ck','bench_lines_sk')\n",
    "ORDER BY table_name, index_name, SEQ_IN_INDEX;\n",
    "\"\"\"\n",
    "\n",
    "index_details_df = pd.DataFrame(run_sql(q_index_details))\n",
    "print(\"\\nIndex details:\")\n",
    "print(index_details_df.to_string(index=False))\n",
    "index_details_df.to_csv(\"index_details.csv\", index=False)\n",
    "\n",
    "print(\"\\n Saved: table_sizes.csv, index_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9667904",
   "metadata": {},
   "source": [
    "##  Table & Index Sizes Benchmarks\n",
    "\n",
    "### Table Sizes\n",
    "| Table            | Data (MB) | Index (MB) | Total (MB) |\n",
    "|------------------|-----------|------------|------------|\n",
    "| `bench_lines_ck` | 98.69     | 0.00       | 98.69      |\n",
    "| `bench_lines_sk` | 109.66    | 75.17      | 184.83     |\n",
    "\n",
    "- **Composite Key (CK)** table is smaller overall because it only has a clustered **PRIMARY KEY** on `(store_sk, day, product_sk, household_sk)`.  \n",
    "- **Surrogate Key (SK)** table consumes almost **2× the space** because:\n",
    "  - The `AUTO_INCREMENT id` primary key is stored in **every secondary index**.\n",
    "  - Additional indexes (`idx_household`, `idx_store_day_prod`) were added to support diverse queries.  \n",
    "\n",
    "---\n",
    "\n",
    "### Index Details\n",
    "- **CK Table (`bench_lines_ck`):**\n",
    "  - One PRIMARY KEY spanning **4 columns** (`store_sk → day → product_sk → household_sk`).\n",
    "  - No secondary indexes defined.\n",
    "  - Efficient for queries aligned with this leftmost prefix (e.g., “sales by store/date window”).\n",
    "  - Not efficient for household-centric lookups without full or wide scans.\n",
    "\n",
    "- **SK Table (`bench_lines_sk`):**\n",
    "  - PRIMARY KEY only on `id` (compact BIGINT).\n",
    "  - Has secondary indexes:\n",
    "    - `idx_household` on `household_sk`\n",
    "    - `idx_store_day_prod` on `(store_sk, day, product_sk)`\n",
    "  - Provides **flexible access paths** for varied queries (household-centric, store/day ranges).\n",
    "  - Larger storage overhead because every secondary index entry carries the `id` PK.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "1. **Storage Tradeoff**  \n",
    "   - CK schema is **leaner** (~99 MB total).  \n",
    "   - SK schema is **heavier** (~185 MB total) due to PK + multiple secondary indexes.  \n",
    "\n",
    "2. **Performance Implications**  \n",
    "   - CK performs best for queries that match its **natural clustering order** (store → day → product → household).  \n",
    "   - SK supports **diverse query patterns** because you can add targeted indexes, at the cost of storage.  \n",
    "\n",
    "3. **Maintainability**  \n",
    "   - SK design is **more flexible**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a733f78",
   "metadata": {},
   "source": [
    "##### NOTE:  for the INSERT throughput I had to do this first to grant permissions : \n",
    "\n",
    "docker exec -it sk_mysql mysql -uroot -p retail_db\n",
    "\n",
    "--sql\n",
    "GRANT CREATE, DROP, INSERT ON retail_db.* TO 'bench'@'%';\n",
    "FLUSH PRIVILEGES;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a6272",
   "metadata": {},
   "source": [
    "## 2) Insert throughput (append vs. scattered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8692fd8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  assignment_task design       ms  rowcount\n",
      "insert_throughput     ck 23433.62   1423952\n",
      "insert_throughput     sk 37728.66   1423952\n",
      "\n",
      " Appended insert timings to benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ---- run insert-throughput inside ONE session so @t0/@t1/@t2 persist ----\n",
    "with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # fresh clones\n",
    "        cur.execute(\"DROP TABLE IF EXISTS bench_ins_ck\")\n",
    "        cur.execute(\"DROP TABLE IF EXISTS bench_ins_sk\")\n",
    "        cur.execute(\"CREATE TABLE bench_ins_ck LIKE bench_lines_ck\")\n",
    "        cur.execute(\"CREATE TABLE bench_ins_sk LIKE bench_lines_sk\")\n",
    "\n",
    "        # time CK insert\n",
    "        cur.execute(\"SET @t0 := NOW(6)\")\n",
    "        cur.execute(\"INSERT INTO bench_ins_ck SELECT * FROM bench_lines_data\")\n",
    "        cur.execute(\"SET @t1 := NOW(6)\")\n",
    "\n",
    "        # time SK insert\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO bench_ins_sk (store_sk, `day`, product_sk, household_sk, sales, qty)\n",
    "            SELECT store_sk, `day`, product_sk, household_sk, sales, qty\n",
    "            FROM bench_lines_data\n",
    "        \"\"\")\n",
    "        cur.execute(\"SET @t2 := NOW(6)\")\n",
    "\n",
    "        # fetch timings\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "              TIMESTAMPDIFF(MICROSECOND,@t0,@t1)/1000.0 AS ms_insert_ck,\n",
    "              TIMESTAMPDIFF(MICROSECOND,@t1,@t2)/1000.0 AS ms_insert_sk\n",
    "        \"\"\")\n",
    "        timings = cur.fetchone()\n",
    "\n",
    "        # rowcounts (nice to include)\n",
    "        cur.execute(\"SELECT COUNT(*) AS n FROM bench_ins_ck\")\n",
    "        n_ck = cur.fetchone()[\"n\"]\n",
    "        cur.execute(\"SELECT COUNT(*) AS n FROM bench_ins_sk\")\n",
    "        n_sk = cur.fetchone()[\"n\"]\n",
    "\n",
    "# build two rows to append\n",
    "rows = [\n",
    "    {\"assignment_task\": \"insert_throughput\", \"design\": \"ck\", \"ms\": round(timings[\"ms_insert_ck\"], 2), \"rowcount\": n_ck},\n",
    "    {\"assignment_task\": \"insert_throughput\", \"design\": \"sk\", \"ms\": round(timings[\"ms_insert_sk\"], 2), \"rowcount\": n_sk},\n",
    "]\n",
    "new_df = pd.DataFrame(rows)\n",
    "\n",
    "# append to existing benchmark_results.csv if present, else create\n",
    "out_path = \"benchmark_results.csv\"\n",
    "if os.path.exists(out_path):\n",
    "    base = pd.read_csv(out_path)\n",
    "    combined = pd.concat([base, new_df], ignore_index=True)\n",
    "else:\n",
    "    combined = new_df\n",
    "\n",
    "# save combined file (overwrite the same CSV for simplicity)\n",
    "combined.to_csv(out_path, index=False)\n",
    "\n",
    "print(new_df.to_string(index=False))\n",
    "print(f\"\\n Appended insert timings to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fd838",
   "metadata": {},
   "source": [
    "## Insert Throughput Benchmark\n",
    "\n",
    "### Results\n",
    "| Design | Insert Time (ms) | Rows Inserted |\n",
    "|--------|------------------:|--------------:|\n",
    "| CK     | 23,433.62         | 1,423,952     |\n",
    "| SK     | 37,728.66         | 1,423,952     |\n",
    "\n",
    "### Observations\n",
    "- **Composite Key (CK)** completed the bulk load in **~23.4 s**, while **Surrogate Key (SK)** took **~37.7 s** — SK is ~**61% slower**, i.e., CK is ~**38% faster** for this load.\n",
    "- Why:\n",
    "  - CK’s clustering on `(store_sk, day, product_sk, household_sk)` fits the input order and updates only the clustered index.\n",
    "  - SK uses an `AUTO_INCREMENT id` **plus two secondary indexes** (`idx_household`, `idx_store_day_prod`), so each insert also updates extra index structures.\n",
    "- **Rowcount matches** for both (1,423,952), confirming correctness.\n",
    "\n",
    "### Conclusion\n",
    "- **CK** is more efficient for large batch loads due to fewer index updates.\n",
    "- **SK** provides flexible query access paths but at a higher **write/maintenance cost** during bulk inserts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349f655",
   "metadata": {},
   "source": [
    "## 3) Random point lookups (household-centric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6d0ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with household_sk = 392\n",
      "\n",
      "--- Composite Key (CK) ---\n",
      "-> Aggregate: sum(bench_lines_ck.qty)  (cost=163175 rows=1) (actual time=1444..1444 rows=1 loops=1)\n",
      "    -> Filter: (bench_lines_ck.household_sk = 392)  (cost=148996 rows=141784) (actual time=1.43..1444 rows=1170 loops=1)\n",
      "        -> Table scan on bench_lines_ck  (cost=148996 rows=1.42e+6) (actual time=1.42..1169 rows=1.42e+6 loops=1)\n",
      "\n",
      "\n",
      "--- Surrogate Key (SK) ---\n",
      "-> Aggregate: sum(bench_lines_sk.qty)  (cost=1404 rows=1) (actual time=7.79..7.79 rows=1 loops=1)\n",
      "    -> Index lookup on bench_lines_sk using idx_household (household_sk=392)  (cost=1287 rows=1170) (actual time=0.764..7.26 rows=1170 loops=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_explain(sql):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            return [row for row in cur.fetchall()]\n",
    "\n",
    "# 1) Pick a household_sk to test\n",
    "with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT household_sk FROM bench_lines_data LIMIT 1;\")\n",
    "        hh = cur.fetchone()[\"household_sk\"]\n",
    "\n",
    "print(f\"Testing with household_sk = {hh}\")\n",
    "\n",
    "# 2) Run EXPLAIN ANALYZE for CK and SK\n",
    "sql_ck = f\"EXPLAIN ANALYZE SELECT SUM(qty) AS total_qty FROM bench_lines_ck WHERE household_sk={hh};\"\n",
    "sql_sk = f\"EXPLAIN ANALYZE SELECT SUM(qty) AS total_qty FROM bench_lines_sk WHERE household_sk={hh};\"\n",
    "\n",
    "plan_ck = run_explain(sql_ck)\n",
    "plan_sk = run_explain(sql_sk)\n",
    "\n",
    "# 3) Pretty print (MySQL returns the execution plan as rows with 'EXPLAIN' key)\n",
    "print(\"\\n--- Composite Key (CK) ---\")\n",
    "for row in plan_ck:\n",
    "    print(row[\"EXPLAIN\"])\n",
    "\n",
    "print(\"\\n--- Surrogate Key (SK) ---\")\n",
    "for row in plan_sk:\n",
    "    print(row[\"EXPLAIN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97042849",
   "metadata": {},
   "source": [
    "## Household-Centric Lookup Benchmark\n",
    "\n",
    "### Results\n",
    "| Design | Execution Time (ms) | Rows Examined | Matching Rows | Result Rows |\n",
    "|--------|--------------------:|--------------:|--------------:|------------:|\n",
    "| CK     | ~1,444              | ~1,420,000    | 1,170         | 1 (SUM)     |\n",
    "| SK     | ~7.8                | ~1,170        | 1,170         | 1 (SUM)     |\n",
    "\n",
    "### Observations\n",
    "- **Composite Key (CK):**\n",
    "  - Performs a **full table scan** (~1.42M rows) because `household_sk` is the last column of the PK\n",
    "    `(store_sk, day, product_sk, household_sk)` and can’t be used via the leftmost-prefix rule.\n",
    "  - The filter then keeps ~1,170 rows, which are aggregated to a single SUM row.\n",
    "  - Total time ≈ **1,444 ms**.\n",
    "\n",
    "- **Surrogate Key (SK):**\n",
    "  - Uses the secondary index **`idx_household`** to jump straight to the ~1,170 matching rows.\n",
    "  - Examines only those index entries and aggregates to a single SUM row.\n",
    "  - Total time ≈ **7.8 ms** — **~185× faster** than CK in this test.\n",
    "\n",
    "### Conclusion\n",
    "- CK is **inefficient** for predicates that don’t align to the PK’s leftmost columns (here, `household_sk`).\n",
    "- SK is **highly flexible**: secondary indexes like `idx_household` provide targeted lookups with minimal IO.\n",
    "- This benchmark highlights the trade-off: **CK = storage/insert efficiency**, **SK = query flexibility & speed** on non-aligned predicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da364a0b",
   "metadata": {},
   "source": [
    "## 4) Leftmost-prefix range scans (CK sweet spot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31f4c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with store_sk=1, day range=1..31\n",
      "\n",
      "--- Composite Key (CK) ---\n",
      "-> Table scan on <temporary>  (actual time=5.54..5.54 rows=0 loops=1)\n",
      "    -> Aggregate using temporary table  (actual time=5.54..5.54 rows=0 loops=1)\n",
      "        -> Filter: ((bench_lines_ck.store_sk = 1) and (bench_lines_ck.`day` between 1 and 31))  (cost=1.21 rows=1) (actual time=2.19..2.19 rows=0 loops=1)\n",
      "            -> Index range scan on bench_lines_ck using PRIMARY over (store_sk = 1 AND 1 <= day <= 31)  (cost=1.21 rows=1) (actual time=0.0614..0.0614 rows=0 loops=1)\n",
      "\n",
      "\n",
      "--- Surrogate Key (SK) ---\n",
      "-> Table scan on <temporary>  (actual time=0.0432..0.0432 rows=0 loops=1)\n",
      "    -> Aggregate using temporary table  (actual time=0.0415..0.0415 rows=0 loops=1)\n",
      "        -> Index range scan on bench_lines_sk using idx_store_day_prod over (store_sk = 1 AND 1 <= day <= 31), with index condition: ((bench_lines_sk.store_sk = 1) and (bench_lines_sk.`day` between 1 and 31))  (cost=2.21 rows=1) (actual time=0.0291..0.0291 rows=0 loops=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_explain(sql):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            return [row[\"EXPLAIN\"] for row in cur.fetchall()]\n",
    "\n",
    "# 1) pick a store and day window\n",
    "with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT store_sk FROM bench_lines_data LIMIT 1;\")\n",
    "        store = cur.fetchone()[\"store_sk\"]\n",
    "        cur.execute(\"SELECT MIN(`day`) AS d0 FROM bench_lines_data;\")\n",
    "        d0 = cur.fetchone()[\"d0\"]\n",
    "        d1 = d0 + 30\n",
    "\n",
    "print(f\"Testing with store_sk={store}, day range={d0}..{d1}\")\n",
    "\n",
    "# 2) build queries\n",
    "sql_ck = f\"\"\"\n",
    "EXPLAIN ANALYZE\n",
    "SELECT product_sk, SUM(sales) AS revenue\n",
    "FROM bench_lines_ck\n",
    "WHERE store_sk={store} AND `day` BETWEEN {d0} AND {d1}\n",
    "GROUP BY product_sk;\n",
    "\"\"\"\n",
    "\n",
    "sql_sk = f\"\"\"\n",
    "EXPLAIN ANALYZE\n",
    "SELECT product_sk, SUM(sales) AS revenue\n",
    "FROM bench_lines_sk\n",
    "WHERE store_sk={store} AND `day` BETWEEN {d0} AND {d1}\n",
    "GROUP BY product_sk;\n",
    "\"\"\"\n",
    "\n",
    "# 3) run and print results\n",
    "plan_ck = run_explain(sql_ck)\n",
    "plan_sk = run_explain(sql_sk)\n",
    "\n",
    "print(\"\\n--- Composite Key (CK) ---\")\n",
    "for line in plan_ck:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n--- Surrogate Key (SK) ---\")\n",
    "for line in plan_sk:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f656e7e",
   "metadata": {},
   "source": [
    "## Store + Day Range Lookup Benchmark\n",
    "\n",
    "**Test:** `store_sk = 1`, `day 1..31`\n",
    "\n",
    "### Results\n",
    "| Design | Execution Time | Access Path |\n",
    "|--------|----------------|-------------|\n",
    "| CK     | **~5.54 s** (range scan node ~**0.061 s**) | Clustered **PRIMARY** range scan on `(store_sk, day, product_sk, household_sk)` |\n",
    "| SK     | **~0.043 s** (range scan node ~**0.029 s**) | Secondary index **`idx_store_day_prod`** on `(store_sk, day, product_sk)` |\n",
    "\n",
    "### Observations\n",
    "- **Both** plans follow an aligned `(store, day)` access path.  \n",
    "- In this run, **SK is much faster overall** (≈**5.54 s** vs **0.043 s** total). Its narrower secondary index returns the needed keys quickly, and the top-level operators finish almost immediately.  \n",
    "- CK does use the clustered PK efficiently (its range scan node is ~61 ms), but the overall query shows **~5.54 s** at the top node, likely reflecting temp/aggregation and/or cache warm-up effects during this sample. Re-running after the buffer pool warms often reduces the CK total substantially.\n",
    "\n",
    "### Conclusion\n",
    "- For **aligned (store + day)** predicates, both designs are capable of efficient index range scans; in this sample the **SK plan won decisively**.  \n",
    "- When reporting, consider both the **range-scan node times** (which were close: 61 ms CK vs 29 ms SK) and the **top-level time** (which can be sensitive to cache state)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac48bf",
   "metadata": {},
   "source": [
    "## 5) Join fanout cost (dim joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed134b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Composite Key (CK) ---\n",
      "-> Limit: 10 row(s)  (actual time=16636..16636 rows=10 loops=1)\n",
      "    -> Sort: revenue DESC, limit input to 10 row(s) per chunk  (actual time=16636..16636 rows=10 loops=1)\n",
      "        -> Table scan on <temporary>  (actual time=16635..16635 rows=62 loops=1)\n",
      "            -> Aggregate using temporary table  (actual time=16635..16635 rows=62 loops=1)\n",
      "                -> Nested loop inner join  (cost=1.71e+6 rows=1.42e+6) (actual time=2.42..3512 rows=1.42e+6 loops=1)\n",
      "                    -> Table scan on b  (cost=148996 rows=1.42e+6) (actual time=2.35..1119 rows=1.42e+6 loops=1)\n",
      "                    -> Single-row index lookup on h using PRIMARY (household_sk=b.household_sk)  (cost=1 rows=1) (actual time=0.00133..0.00139 rows=1 loops=1.42e+6)\n",
      "\n",
      "\n",
      "--- Surrogate Key (SK) ---\n",
      "-> Limit: 10 row(s)  (actual time=14531..14531 rows=10 loops=1)\n",
      "    -> Sort: revenue DESC, limit input to 10 row(s) per chunk  (actual time=14531..14531 rows=10 loops=1)\n",
      "        -> Table scan on <temporary>  (actual time=14531..14531 rows=62 loops=1)\n",
      "            -> Aggregate using temporary table  (actual time=14531..14531 rows=62 loops=1)\n",
      "                -> Nested loop inner join  (cost=645756 rows=1.42e+6) (actual time=1.56..3006 rows=1.42e+6 loops=1)\n",
      "                    -> Table scan on b  (cost=149756 rows=1.42e+6) (actual time=1.54..957 rows=1.42e+6 loops=1)\n",
      "                    -> Single-row index lookup on h using PRIMARY (household_sk=b.household_sk)  (cost=0.25 rows=1) (actual time=0.00113..0.00118 rows=1 loops=1.42e+6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_explain(sql):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            return [row[\"EXPLAIN\"] for row in cur.fetchall()]\n",
    "\n",
    "# CK query\n",
    "sql_ck = \"\"\"\n",
    "EXPLAIN ANALYZE\n",
    "SELECT h.income_desc, h.age_desc, SUM(b.sales) AS revenue\n",
    "FROM bench_lines_ck b\n",
    "JOIN household_dim h ON h.household_sk = b.household_sk\n",
    "GROUP BY h.income_desc, h.age_desc\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# SK query\n",
    "sql_sk = \"\"\"\n",
    "EXPLAIN ANALYZE\n",
    "SELECT h.income_desc, h.age_desc, SUM(b.sales) AS revenue\n",
    "FROM bench_lines_sk b\n",
    "JOIN household_dim h ON h.household_sk = b.household_sk\n",
    "GROUP BY h.income_desc, h.age_desc\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# Run and print results\n",
    "plan_ck = run_explain(sql_ck)\n",
    "plan_sk = run_explain(sql_sk)\n",
    "\n",
    "print(\"\\n--- Composite Key (CK) ---\")\n",
    "for line in plan_ck:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n--- Surrogate Key (SK) ---\")\n",
    "for line in plan_sk:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2bdcb",
   "metadata": {},
   "source": [
    "## Demographic Influence Benchmark\n",
    "\n",
    "### Results\n",
    "| Design | Execution Time | Access Path |\n",
    "|--------|----------------|-------------|\n",
    "| CK     | ~16,636 ms     | Full table scan of fact table (`bench_lines_ck`), nested loop join to `household_dim` (PK lookup), aggregate + sort, LIMIT 10 |\n",
    "| SK     | ~14,531 ms     | Full table scan of fact table (`bench_lines_sk`), nested loop join to `household_dim` (PK lookup), aggregate + sort, LIMIT 10 |\n",
    "\n",
    "### Observations\n",
    "- Both plans **scan ~1.42M fact rows**, join each to `household_dim` via its **PRIMARY KEY**, then **aggregate and sort**.\n",
    "- **SK is ~13% faster** in this run (14.53s vs 16.64s). The difference is modest and likely reflects cache/warm-up and row layout effects; structurally the plans are the same.\n",
    "- Nested loop lookups into `household_dim` are cheap in both (single-row PK lookups).\n",
    "\n",
    "### Conclusion\n",
    "- This workload is **scan + group dominated**, so neither design has a strong structural advantage.\n",
    "- Expect small run-to-run variation; with a warm buffer pool the gap may narrow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e23ecf",
   "metadata": {},
   "source": [
    "## 6) Covering-index vs. back-to-table lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01be5542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK index already exists (or error): (1142, \"ALTER command denied to user 'bench'@'172.24.0.1' for table 'bench_lines_sk'\")\n",
      "CK index already exists (or error): (1142, \"ALTER command denied to user 'bench'@'172.24.0.1' for table 'bench_lines_ck'\")\n",
      "Testing with store_sk=1, day range=1..31\n",
      "\n",
      "--- Surrogate Key (SK) with covering index ---\n",
      "-> Table scan on <temporary>  (actual time=0.0257..0.0257 rows=0 loops=1)\n",
      "    -> Aggregate using temporary table  (actual time=0.0243..0.0243 rows=0 loops=1)\n",
      "        -> Index range scan on bench_lines_sk using idx_store_day_prod over (store_sk = 1 AND 1 <= day <= 31), with index condition: ((bench_lines_sk.store_sk = 1) and (bench_lines_sk.`day` between 1 and 31))  (cost=2.21 rows=1) (actual time=0.0158..0.0158 rows=0 loops=1)\n",
      "\n",
      "\n",
      "--- Composite Key (CK) with covering index ---\n",
      "-> Table scan on <temporary>  (actual time=0.0434..0.0434 rows=0 loops=1)\n",
      "    -> Aggregate using temporary table  (actual time=0.042..0.042 rows=0 loops=1)\n",
      "        -> Filter: ((bench_lines_ck.store_sk = 1) and (bench_lines_ck.`day` between 1 and 31))  (cost=1.21 rows=1) (actual time=0.0311..0.0311 rows=0 loops=1)\n",
      "            -> Index range scan on bench_lines_ck using PRIMARY over (store_sk = 1 AND 1 <= day <= 31)  (cost=1.21 rows=1) (actual time=0.03..0.03 rows=0 loops=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_explain(sql):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            return [row[\"EXPLAIN\"] for row in cur.fetchall()]\n",
    "\n",
    "# 1) Add covering indexes (safe to re-run, MySQL will error if already exists)\n",
    "with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            cur.execute(\"ALTER TABLE bench_lines_sk ADD KEY cov_store_day_prod_sales (store_sk, `day`, product_sk, sales)\")\n",
    "        except Exception as e:\n",
    "            print(\"SK index already exists (or error):\", e)\n",
    "        try:\n",
    "            cur.execute(\"ALTER TABLE bench_lines_ck ADD KEY cov_store_day_prod_sales (store_sk, `day`, product_sk, sales)\")\n",
    "        except Exception as e:\n",
    "            print(\"CK index already exists (or error):\", e)\n",
    "\n",
    "# 2) Pick a store and date window\n",
    "with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT store_sk FROM bench_lines_data LIMIT 1;\")\n",
    "        store = cur.fetchone()[\"store_sk\"]\n",
    "        cur.execute(\"SELECT MIN(`day`) AS d0 FROM bench_lines_data;\")\n",
    "        d0 = cur.fetchone()[\"d0\"]\n",
    "        d1 = d0 + 30\n",
    "\n",
    "print(f\"Testing with store_sk={store}, day range={d0}..{d1}\")\n",
    "\n",
    "# 3) Build queries\n",
    "sql_sk = f\"\"\"\n",
    "EXPLAIN ANALYZE\n",
    "SELECT product_sk, SUM(sales) AS revenue\n",
    "FROM bench_lines_sk\n",
    "WHERE store_sk={store} AND `day` BETWEEN {d0} AND {d1}\n",
    "GROUP BY product_sk;\n",
    "\"\"\"\n",
    "\n",
    "sql_ck = f\"\"\"\n",
    "EXPLAIN ANALYZE\n",
    "SELECT product_sk, SUM(sales) AS revenue\n",
    "FROM bench_lines_ck\n",
    "WHERE store_sk={store} AND `day` BETWEEN {d0} AND {d1}\n",
    "GROUP BY product_sk;\n",
    "\"\"\"\n",
    "\n",
    "# 4) Run and print results\n",
    "plan_sk = run_explain(sql_sk)\n",
    "plan_ck = run_explain(sql_ck)\n",
    "\n",
    "print(\"\\n--- Surrogate Key (SK) with covering index ---\")\n",
    "for line in plan_sk:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n--- Composite Key (CK) with covering index ---\")\n",
    "for line in plan_ck:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a31a9",
   "metadata": {},
   "source": [
    "## Covering Index Benchmark (Spend Trends)\n",
    "\n",
    "**Note:** The `ALTER ... ADD KEY cov_store_day_prod_sales` statements were **denied**, so this run used the **existing** indexes (no new covering columns were added).\n",
    "\n",
    "**Test:** `store_sk = 1`, `day 1..31`\n",
    "\n",
    "### Results\n",
    "| Design | Top-node time | Range-scan node time | Access Path |\n",
    "|--------|---------------:|---------------------:|-------------|\n",
    "| SK     | **~0.0257 s**  | ~0.0158 s            | Secondary index **`idx_store_day_prod (store, day, product)`** |\n",
    "| CK     | ~0.0434 s      | ~0.0300 s            | Clustered **PRIMARY** `(store, day, product, household)` |\n",
    "\n",
    "### Observations\n",
    "- Both plans are **aligned on `(store, day)`** and are therefore extremely fast.\n",
    "- In this sample, **SK is ~40% faster** overall (25.7 ms vs 43.4 ms). Its narrower secondary index scans slightly fewer bytes.\n",
    "- CK uses the clustered PRIMARY range scan efficiently as well; the higher top-node time likely reflects minor temp/aggregation + cache effects.\n",
    "- The plan shows **“Aggregate using temporary table”** for both, which is expected for `GROUP BY product_sk`.\n",
    "\n",
    "### Conclusion\n",
    "- For aligned predicates, **both CK and SK deliver sub-50 ms performance** with index range scans.\n",
    "- **SK edged out CK** here using `idx_store_day_prod`; **CK** still performs well without needing extra secondary indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21507119",
   "metadata": {},
   "source": [
    "## 7) Secondary index count & maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "253b613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection info:\n",
      "        user_host curr_user  conn_id\n",
      "bench@172.24.0.1   bench@%     3557\n",
      "\n",
      "--- SHOW INDEX: bench_lines_ck ---\n",
      "         Table  Non_unique Key_name  Seq_in_index  Column_name Collation  Cardinality Sub_part Packed Null Index_type Comment Index_comment Visible Expression\n",
      "bench_lines_ck           0  PRIMARY             1     store_sk         A          407     None   None           BTREE                           YES       None\n",
      "bench_lines_ck           0  PRIMARY             2          day         A        52770     None   None           BTREE                           YES       None\n",
      "bench_lines_ck           0  PRIMARY             3   product_sk         A      1410659     None   None           BTREE                           YES       None\n",
      "bench_lines_ck           0  PRIMARY             4 household_sk         A      1417842     None   None           BTREE                           YES       None\n",
      "\n",
      "--- SHOW INDEX: bench_lines_sk ---\n",
      "         Table  Non_unique           Key_name  Seq_in_index  Column_name Collation  Cardinality Sub_part Packed Null Index_type Comment Index_comment Visible Expression\n",
      "bench_lines_sk           0            PRIMARY             1           id         A      1417143     None   None           BTREE                           YES       None\n",
      "bench_lines_sk           1 idx_store_day_prod             1     store_sk         A          393     None   None           BTREE                           YES       None\n",
      "bench_lines_sk           1 idx_store_day_prod             2          day         A        53800     None   None  YES      BTREE                           YES       None\n",
      "bench_lines_sk           1 idx_store_day_prod             3   product_sk         A      1411597     None   None           BTREE                           YES       None\n",
      "bench_lines_sk           1      idx_household             1 household_sk         A          796     None   None           BTREE                           YES       None\n",
      "\n",
      "--- information_schema.STATISTICS ---\n",
      "    TABLE_NAME         INDEX_NAME  NON_UNIQUE  SEQ_IN_INDEX  COLUMN_NAME SUB_PART INDEX_TYPE  CARDINALITY\n",
      "bench_lines_ck            PRIMARY           0             1     store_sk     None      BTREE          407\n",
      "bench_lines_ck            PRIMARY           0             2          day     None      BTREE        52770\n",
      "bench_lines_ck            PRIMARY           0             3   product_sk     None      BTREE      1410659\n",
      "bench_lines_ck            PRIMARY           0             4 household_sk     None      BTREE      1417842\n",
      "bench_lines_sk      idx_household           1             1 household_sk     None      BTREE          796\n",
      "bench_lines_sk idx_store_day_prod           1             1     store_sk     None      BTREE          393\n",
      "bench_lines_sk idx_store_day_prod           1             2          day     None      BTREE        53800\n",
      "bench_lines_sk idx_store_day_prod           1             3   product_sk     None      BTREE      1411597\n",
      "bench_lines_sk            PRIMARY           0             1           id     None      BTREE      1417143\n",
      "\n",
      " Saved: bench_lines_ck_show_index.csv, bench_lines_sk_show_index.csv, index_details.csv\n"
     ]
    }
   ],
   "source": [
    "import pymysql, pandas as pd\n",
    "\n",
    "#  Use bench (root remote is blocked in your container)\n",
    "CONN = dict(\n",
    "    host=\"127.0.0.1\",\n",
    "    port=3307,\n",
    "    user=\"bench\",\n",
    "    password=\"benchpw\",\n",
    "    database=\"retail_db\"\n",
    ")\n",
    "\n",
    "def fetch_df(sql):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- Sanity check (avoid confusing aliases) ---\n",
    "try:\n",
    "    who = fetch_df(\"SELECT USER() AS user_host, CURRENT_USER() AS curr_user, CONNECTION_ID() AS conn_id\")\n",
    "    print(\"Connection info:\\n\", who.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"Sanity check failed:\", e)\n",
    "    print(fetch_df(\"SELECT 1 AS ok\").to_string(index=False))\n",
    "\n",
    "# --- SHOW INDEX (raw MySQL output) ---\n",
    "ck_show = fetch_df(\"SHOW INDEX FROM bench_lines_ck\")\n",
    "sk_show = fetch_df(\"SHOW INDEX FROM bench_lines_sk\")\n",
    "\n",
    "print(\"\\n--- SHOW INDEX: bench_lines_ck ---\")\n",
    "print(ck_show.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- SHOW INDEX: bench_lines_sk ---\")\n",
    "print(sk_show.to_string(index=False))\n",
    "\n",
    "ck_show.to_csv(\"bench_lines_ck_show_index.csv\", index=False)\n",
    "sk_show.to_csv(\"bench_lines_sk_show_index.csv\", index=False)\n",
    "\n",
    "# --- information_schema.STATISTICS (clean detail view) ---\n",
    "q_stats = \"\"\"\n",
    "SELECT \n",
    "  TABLE_NAME, INDEX_NAME, NON_UNIQUE, SEQ_IN_INDEX, COLUMN_NAME, SUB_PART, INDEX_TYPE, CARDINALITY\n",
    "FROM information_schema.STATISTICS\n",
    "WHERE TABLE_SCHEMA = DATABASE()\n",
    "  AND TABLE_NAME IN ('bench_lines_ck','bench_lines_sk')\n",
    "ORDER BY TABLE_NAME, INDEX_NAME, SEQ_IN_INDEX\n",
    "\"\"\"\n",
    "idx_details = fetch_df(q_stats)\n",
    "\n",
    "print(\"\\n--- information_schema.STATISTICS ---\")\n",
    "print(idx_details.to_string(index=False))\n",
    "\n",
    "idx_details.to_csv(\"index_details.csv\", index=False)\n",
    "print(\"\\n Saved: bench_lines_ck_show_index.csv, bench_lines_sk_show_index.csv, index_details.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34978638",
   "metadata": {},
   "source": [
    "## Index Inventory (CK vs SK) — from `SHOW INDEX`\n",
    "\n",
    "### `bench_lines_ck` (Composite Key)\n",
    "| Index   | Columns (order)                                             | Type  | Cardinality (est.)                    |\n",
    "|---------|--------------------------------------------------------------|-------|---------------------------------------|\n",
    "| PRIMARY | `store_sk` → `day` → `product_sk` → `household_sk`           | BTREE | 407 / 52,770 / 1,410,659 / 1,417,842  |\n",
    "\n",
    "> **Only one** clustered PRIMARY KEY; **no secondary indexes**.\n",
    "\n",
    "---\n",
    "\n",
    "### `bench_lines_sk` (Surrogate Key)\n",
    "| Index              | Columns (order)                        | Type  | Cardinality (est.)      |\n",
    "|--------------------|----------------------------------------|-------|-------------------------|\n",
    "| PRIMARY            | `id`                                   | BTREE | 1,417,143               |\n",
    "| `idx_store_day_prod` | `store_sk` → `day` → `product_sk`    | BTREE | 393 / 53,800 / 1,411,597|\n",
    "| `idx_household`    | `household_sk`                         | BTREE | 796                     |\n",
    "\n",
    "> **Three** indexes total: compact PK (`id`) **+ 2 secondaries** that power store/day and household access paths.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this matters \n",
    "\n",
    "- **Index count & size**\n",
    "  - CK: 1 index → smaller footprint (**~98.7 MB** total).\n",
    "  - SK: 3 indexes → larger footprint (**~184.8 MB** total).\n",
    "\n",
    "- **Write & maintenance overhead**\n",
    "  - **Bulk insert (≈1.42M rows):** CK **23,433 ms** vs SK **37,729 ms** (SK slower due to extra index maintenance).\n",
    "  - **OPTIMIZE/REBUILD:** CK **27 s** vs SK **49 s** (SK ~1.8× slower).\n",
    "\n",
    "- **Query behavior explained**\n",
    "  - **Store + day** workloads:  \n",
    "    - CK uses clustered PK prefix → very fast.  \n",
    "    - SK uses `idx_store_day_prod` → similarly fast (needs the secondary index).\n",
    "  - **Household-centric lookups:**  \n",
    "    - CK lacks an index on `household_sk` alone → scan-heavy.  \n",
    "    - SK uses `idx_household` → direct index lookup (orders of magnitude faster).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89f705",
   "metadata": {},
   "source": [
    "## 8) Buffer pool friendliness (rough check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "621544b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: household_sk=392, store_sk=1, day range 1..31\n",
      "\n",
      "InnoDB buffer-pool counters (diffs show impact of each heavy query):\n",
      "                                        baseline   after_CK  after_CK_minus_before   after_SK  after_SK_minus_after_CK\n",
      "Innodb_buffer_pool_read_ahead_rnd              0          0                      0          0                        0\n",
      "Innodb_buffer_pool_read_ahead             553693     553821                    128     553821                        0\n",
      "Innodb_buffer_pool_read_ahead_evicted       3248       3345                     97       3345                        0\n",
      "Innodb_buffer_pool_read_requests       286958218  286984248                  26030  286992357                     8109\n",
      "Innodb_buffer_pool_reads                  283510     296000                  12490     296089                       89\n",
      "\n",
      " Saved buffer pool diffs to buffer_pool_diffs.csv\n",
      "\n",
      "(sys.schema_table_statistics not accessible?) (1142, \"SELECT command denied to user 'bench'@'172.24.0.1' for table 'schema_table_statistics'\")\n",
      "You can run this inside the container as root if needed:\n",
      "  SELECT * FROM sys.schema_table_statistics WHERE table_schema='retail_db' AND table_name IN ('bench_lines_ck','bench_lines_sk') ORDER BY rows_fetched DESC LIMIT 10;\n"
     ]
    }
   ],
   "source": [
    "def fetch(sql, one=False):\n",
    "    with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            return cur.fetchone() if one else cur.fetchall()\n",
    "\n",
    "def get_bp_stats():\n",
    "    rows = fetch(\"SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_read%';\")\n",
    "    # Rows come back as [{'Variable_name': 'Innodb_buffer_pool_read_requests', 'Value': '...'}, ...]\n",
    "    d = {r[\"Variable_name\"]: int(r[\"Value\"]) for r in rows}\n",
    "    return pd.Series(d, name=\"bp\")\n",
    "\n",
    "def run(sql):\n",
    "    # Execute a statement and return the first row (if any)\n",
    "    return fetch(sql, one=True)\n",
    "\n",
    "# Pick a household and a store/day window\n",
    "hh = fetch(\"SELECT household_sk FROM bench_lines_data LIMIT 1\", one=True)[\"household_sk\"]\n",
    "d0 = fetch(\"SELECT MIN(`day`) AS d0 FROM bench_lines_data\", one=True)[\"d0\"]\n",
    "d1 = d0 + 30\n",
    "store = fetch(\"SELECT store_sk FROM bench_lines_data LIMIT 1\", one=True)[\"store_sk\"]\n",
    "print(f\"Using: household_sk={hh}, store_sk={store}, day range {d0}..{d1}\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Buffer-pool baseline\n",
    "# -------------------------\n",
    "bp0 = get_bp_stats()\n",
    "\n",
    "# -------------------------\n",
    "# 2) Heavy CK query (forces scan)\n",
    "# -------------------------\n",
    "# household filter on CK (household_sk is the 4th PK column -> table scan)\n",
    "run(f\"EXPLAIN ANALYZE SELECT SUM(qty) FROM bench_lines_ck WHERE household_sk={hh}\")\n",
    "run(f\"SELECT SUM(qty) FROM bench_lines_ck WHERE household_sk={hh}\")\n",
    "\n",
    "bp1 = get_bp_stats()\n",
    "\n",
    "# -------------------------\n",
    "# 3) Heavy SK query (uses index on household)\n",
    "# -------------------------\n",
    "run(f\"EXPLAIN ANALYZE SELECT SUM(qty) FROM bench_lines_sk WHERE household_sk={hh}\")\n",
    "run(f\"SELECT SUM(qty) FROM bench_lines_sk WHERE household_sk={hh}\")\n",
    "\n",
    "bp2 = get_bp_stats()\n",
    "\n",
    "# Build comparison table\n",
    "ck_diff = (bp1 - bp0).rename(\"after_CK_minus_before\")\n",
    "sk_diff = (bp2 - bp1).rename(\"after_SK_minus_after_CK\")\n",
    "cmp_df = pd.concat([bp0.rename(\"baseline\"), bp1.rename(\"after_CK\"), ck_diff, bp2.rename(\"after_SK\"), sk_diff], axis=1)\n",
    "print(\"\\nInnoDB buffer-pool counters (diffs show impact of each heavy query):\")\n",
    "print(cmp_df.to_string())\n",
    "\n",
    "# Save if you want to include in report\n",
    "cmp_df.to_csv(\"buffer_pool_diffs.csv\")\n",
    "print(\"\\n Saved buffer pool diffs to buffer_pool_diffs.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Per-table stats via sys\n",
    "# -------------------------\n",
    "q_sys = \"\"\"\n",
    "SELECT *\n",
    "FROM sys.schema_table_statistics\n",
    "WHERE table_schema='retail_db'\n",
    "  AND table_name IN ('bench_lines_ck','bench_lines_sk')\n",
    "ORDER BY rows_fetched DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "try:\n",
    "    sys_df = pd.DataFrame(fetch(q_sys))\n",
    "    print(\"\\nPer-table stats (sys.schema_table_statistics):\")\n",
    "    print(sys_df.to_string(index=False))\n",
    "    sys_df.to_csv(\"schema_table_statistics.csv\", index=False)\n",
    "    print(\"\\n Saved schema_table_statistics.csv\")\n",
    "except Exception as e:\n",
    "    print(\"\\n(sys.schema_table_statistics not accessible?)\", e)\n",
    "    print(\"You can run this inside the container as root if needed:\\n\"\n",
    "          \"  SELECT * FROM sys.schema_table_statistics WHERE table_schema='retail_db' \"\n",
    "          \"AND table_name IN ('bench_lines_ck','bench_lines_sk') ORDER BY rows_fetched DESC LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ef805",
   "metadata": {},
   "source": [
    "## InnoDB Buffer-Pool Impact (CK vs SK — household lookup)\n",
    "\n",
    "**Context**  \n",
    "CK query = `SUM(qty) FROM bench_lines_ck WHERE household_sk=?` (no useful index → table scan)  \n",
    "SK query = `SUM(qty) FROM bench_lines_sk WHERE household_sk=?` (uses `idx_household`)  \n",
    "Run with: `household_sk=392`, `store_sk=1`, `day 1..31`\n",
    "\n",
    "### Deltas in Buffer-Pool Counters\n",
    "| Metric                                   | CK Δ (after_CK − baseline) | SK Δ (after_SK − after_CK) |\n",
    "|------------------------------------------|----------------------------:|---------------------------:|\n",
    "| Innodb_buffer_pool_read_requests         | **+26,030**                 | **+8,109**                 |\n",
    "| Innodb_buffer_pool_reads *(disk misses)* | **+12,490**                 | **+89**                    |\n",
    "| Innodb_buffer_pool_read_ahead            | **+128**                    | **0**                      |\n",
    "| Innodb_buffer_pool_read_ahead_evicted    | **+97**                     | **0**                      |\n",
    "| Innodb_buffer_pool_read_ahead_rnd        | 0                           | 0                          |\n",
    "\n",
    "### What this shows\n",
    "- The **CK** household lookup triggers a **large scan**, causing ~**3.2×** more logical requests and ~**140×** more disk reads (12,490 vs 89). Read-ahead activity confirms sequential scanning.\n",
    "- The **SK** lookup uses the `idx_household` secondary index, touching far fewer pages and causing almost no misses — **much more cache-friendly**.\n",
    "\n",
    "### Conclusion\n",
    "For household-centric queries, **SK is dramatically more efficient** in buffer usage and IO due to targeted secondary indexing. CK remains space-lean, but when predicates don’t align with its leftmost PK prefix, it becomes **scan-heavy**.\n",
    "\n",
    "> Absolute counts depend on cache warm-up; the **relative gap** (CK ≫ SK) is the key signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f429f5e",
   "metadata": {},
   "source": [
    "## 9) DELETE/PURGE behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda4694",
   "metadata": {},
   "source": [
    "#### Need to run the below before doing this to get DEL access -\n",
    "docker exec -it sk_mysql mysql -uroot -p -e ^ </br>\n",
    "\"GRANT CREATE, INSERT, DELETE, DROP ON retail_db.* TO 'bench'@'%'; FLUSH PRIVILEGES;\"\n",
    "\n",
    "Verify using: </br>\n",
    "docker exec -it sk_mysql mysql -uroot -p -e \"SHOW GRANTS FOR 'bench'@'%';\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06c818aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store 140, delete window: day 6..36\n",
      "assignment_task design     ms  rowcount\n",
      "  delete_window     ck  91.00       208\n",
      "  delete_window     sk 118.51       208\n",
      "\n",
      " Appended delete timings to benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # 1) choose a store with lots of data (top 1 by rows)\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT store_sk\n",
    "            FROM (\n",
    "              SELECT store_sk, COUNT(*) AS c\n",
    "              FROM bench_lines_data\n",
    "              GROUP BY store_sk\n",
    "              ORDER BY c DESC\n",
    "              LIMIT 1\n",
    "            ) t\n",
    "        \"\"\")\n",
    "        store = cur.fetchone()[\"store_sk\"]\n",
    "\n",
    "        # 2) build tiny working copies (fresh each run)\n",
    "        cur.execute(\"DROP TABLE IF EXISTS del_ck\")\n",
    "        cur.execute(\"DROP TABLE IF EXISTS del_sk\")\n",
    "        cur.execute(\"CREATE TABLE del_ck LIKE bench_lines_ck\")\n",
    "        cur.execute(\"CREATE TABLE del_sk LIKE bench_lines_sk\")\n",
    "        cur.execute(f\"INSERT INTO del_ck SELECT * FROM bench_lines_ck WHERE store_sk={store} LIMIT 50000\")\n",
    "        cur.execute(f\"\"\"\n",
    "            INSERT INTO del_sk (id, store_sk, `day`, product_sk, household_sk, sales, qty)\n",
    "            SELECT NULL, store_sk, `day`, product_sk, household_sk, sales, qty\n",
    "            FROM bench_lines_sk\n",
    "            WHERE store_sk={store}\n",
    "            LIMIT 50000\n",
    "        \"\"\")\n",
    "\n",
    "        # Count rows we copied\n",
    "        cur.execute(\"SELECT COUNT(*) AS n FROM del_ck\"); n_ck_before = cur.fetchone()[\"n\"]\n",
    "        cur.execute(\"SELECT COUNT(*) AS n FROM del_sk\"); n_sk_before = cur.fetchone()[\"n\"]\n",
    "\n",
    "        # 3) pick a 30-day window **from the data we just copied** (ensures some deletions happen)\n",
    "        cur.execute(\"SELECT MIN(`day`) AS d0 FROM del_ck\")\n",
    "        d0 = cur.fetchone()[\"d0\"]\n",
    "        d1 = d0 + 30\n",
    "\n",
    "        # 4) time deletes on the server and capture affected rows\n",
    "        cur.execute(\"SET @t0 := NOW(6)\")\n",
    "        cur.execute(f\"DELETE FROM del_ck WHERE `day` BETWEEN {d0} AND {d1}\")\n",
    "        cur.execute(\"SET @d_ck := ROW_COUNT()\")\n",
    "        cur.execute(\"SET @t1 := NOW(6)\")\n",
    "\n",
    "        cur.execute(f\"DELETE FROM del_sk WHERE `day` BETWEEN {d0} AND {d1}\")\n",
    "        cur.execute(\"SET @d_sk := ROW_COUNT()\")\n",
    "        cur.execute(\"SET @t2 := NOW(6)\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "              TIMESTAMPDIFF(MICROSECOND,@t0,@t1)/1000.0 AS ms_del_ck,\n",
    "              @d_ck AS rows_del_ck,\n",
    "              TIMESTAMPDIFF(MICROSECOND,@t1,@t2)/1000.0 AS ms_del_sk,\n",
    "              @d_sk AS rows_del_sk\n",
    "        \"\"\")\n",
    "        res = cur.fetchone()\n",
    "\n",
    "# Print results\n",
    "print(f\"Store {store}, delete window: day {d0}..{d1}\")\n",
    "print(pd.DataFrame([{\n",
    "    \"assignment_task\": \"delete_window\",\n",
    "    \"design\": \"ck\",\n",
    "    \"ms\": round(res['ms_del_ck'], 2),\n",
    "    \"rowcount\": int(res['rows_del_ck'])\n",
    "}, {\n",
    "    \"assignment_task\": \"delete_window\",\n",
    "    \"design\": \"sk\",\n",
    "    \"ms\": round(res['ms_del_sk'], 2),\n",
    "    \"rowcount\": int(res['rows_del_sk'])\n",
    "}]).to_string(index=False))\n",
    "\n",
    "# 5) append to benchmark_results.csv\n",
    "rows = [\n",
    "    {\"assignment_task\": \"delete_window\", \"design\": \"ck\", \"ms\": round(res['ms_del_ck'], 2), \"rowcount\": int(res['rows_del_ck'])},\n",
    "    {\"assignment_task\": \"delete_window\", \"design\": \"sk\", \"ms\": round(res['ms_del_sk'], 2), \"rowcount\": int(res['rows_del_sk'])},\n",
    "]\n",
    "new_df = pd.DataFrame(rows)\n",
    "\n",
    "out_path = \"delete-purge_results.csv\"\n",
    "if os.path.exists(out_path):\n",
    "    base = pd.read_csv(out_path)\n",
    "    combined = pd.concat([base, new_df], ignore_index=True)\n",
    "else:\n",
    "    combined = new_df\n",
    "combined.to_csv(out_path, index=False)\n",
    "print(f\"\\n Appended delete timings to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c53a3",
   "metadata": {},
   "source": [
    "## Delete Range Benchmark\n",
    "\n",
    "**Setup:** Copied ~50k rows per design into `del_ck` / `del_sk` for one store, then deleted a 30-day window.  \n",
    "**Window:** `store = 140`, `days 6..36`\n",
    "\n",
    "### Results\n",
    "| Design | Delete Time (ms) | Rows Deleted |\n",
    "|--------|------------------:|-------------:|\n",
    "| CK     | **91.00**         | 208          |\n",
    "| SK     | 118.51            | 208          |\n",
    "\n",
    "### Observations\n",
    "- Both designs perform similarly for this **localized** delete (only ~208 rows).\n",
    "- **CK is ~1.30× faster** (91 ms vs 118.5 ms). Rows are clustered by `(store_sk, day, …)`, so the range maps to a contiguous span.\n",
    "- **SK** must also update two secondary indexes (`idx_store_day_prod`, `idx_household`), adding a small overhead.\n",
    "\n",
    "### Takeaways\n",
    "- For **range deletes aligned** with `(store, day)`, CK can be modestly faster.\n",
    "- On SK, keep the predicate selective (include `store_sk`) so the optimizer can use `idx_store_day_prod`.\n",
    "- For **larger deletions**, batch in chunks (e.g., `LIMIT 10k` in a loop) or consider day partitioning to reduce locks and purge work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610331e",
   "metadata": {},
   "source": [
    "## 10) Table rebuild/OPTIMIZE time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc57817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZE messages (CK):\n",
      "                   Table       Op Msg_type                                                          Msg_text\n",
      "retail_db.bench_lines_ck optimize     note Table does not support optimize, doing recreate + analyze instead\n",
      "retail_db.bench_lines_ck optimize   status                                                                OK\n",
      "\n",
      "OPTIMIZE messages (SK):\n",
      "                   Table       Op Msg_type                                                          Msg_text\n",
      "retail_db.bench_lines_sk optimize     note Table does not support optimize, doing recreate + analyze instead\n",
      "retail_db.bench_lines_sk optimize   status                                                                OK\n",
      "\n",
      "Timings (seconds):\n",
      " sec_opt_ck  sec_opt_sk\n",
      "         25          53\n",
      "\n",
      " Saved to optimize_times.csv\n"
     ]
    }
   ],
   "source": [
    "with pymysql.connect(**CONN, cursorclass=pymysql.cursors.DictCursor, autocommit=True) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SET @t0 := NOW(6)\")\n",
    "        cur.execute(\"OPTIMIZE TABLE bench_lines_ck\")\n",
    "        ck_msg = cur.fetchall()  # [{'Table':..., 'Op':'optimize', 'Msg_type':'status', 'Msg_text':'OK'|...}]\n",
    "\n",
    "        cur.execute(\"SET @t1 := NOW(6)\")\n",
    "        cur.execute(\"OPTIMIZE TABLE bench_lines_sk\")\n",
    "        sk_msg = cur.fetchall()\n",
    "\n",
    "        cur.execute(\"SET @t2 := NOW(6)\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "              TIMESTAMPDIFF(SECOND,@t0,@t1) AS sec_opt_ck,\n",
    "              TIMESTAMPDIFF(SECOND,@t1,@t2) AS sec_opt_sk\n",
    "        \"\"\")\n",
    "        times = pd.DataFrame(cur.fetchall())\n",
    "\n",
    "print(\"OPTIMIZE messages (CK):\")\n",
    "print(pd.DataFrame(ck_msg).to_string(index=False))\n",
    "print(\"\\nOPTIMIZE messages (SK):\")\n",
    "print(pd.DataFrame(sk_msg).to_string(index=False))\n",
    "\n",
    "print(\"\\nTimings (seconds):\")\n",
    "print(times.to_string(index=False))\n",
    "times.to_csv(\"optimize_times.csv\", index=False)\n",
    "print(\"\\n Saved to optimize_times.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22eae88",
   "metadata": {},
   "source": [
    "## Table Maintenance Benchmark — `OPTIMIZE TABLE`\n",
    "\n",
    "**MySQL output:**  \n",
    "> *“Table does not support optimize, doing recreate + analyze instead”*  \n",
    "For InnoDB this means MySQL **rebuilds** the table and **ANALYZE**s its indexes (locks table for the duration).\n",
    "\n",
    "### Results\n",
    "| Table             | Time (s) | Notes |\n",
    "|-------------------|---------:|------|\n",
    "| `bench_lines_ck`  | **27**   | Fewer/lighter indexes to rebuild |\n",
    "| `bench_lines_sk`  | **49**   | More/larger secondary indexes → slower |\n",
    "\n",
    "### Observations\n",
    "- The **SK** design takes ~**1.8×** longer to rebuild/analyze than **CK**.  \n",
    "- This aligns with earlier size findings (SK ≈ **185 MB** vs CK ≈ **99 MB**) and SK’s extra secondary indexes; more bytes and indexes to recreate ⇒ longer maintenance.\n",
    "\n",
    "### Takeaways\n",
    "- **Maintenance overhead:** SK is heavier for housekeeping tasks (OPTIMIZE/ALTER/REBUILD) due to index fan-out and size.  \n",
    "- **When to run:** after large bulk **DELETE/INSERT** waves or periodic housekeeping to defragment/reclaim per-table space.  \n",
    "- **Operational note:** expect **blocking** during the rebuild; schedule during low-traffic windows or use partitioning/online DDL strategies if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09944d5e",
   "metadata": {},
   "source": [
    "##  One combined Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaca2922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wrote retail_benchmark_report.xlsx with sheets: benchmarks, table_sizes, index_details, indexes_ck, indexes_sk, buffer_pool, optimize_times, meta\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "\n",
    "# Where to save the Excel workbook\n",
    "OUT_XLSX = \"retail_benchmark_report.xlsx\"\n",
    "\n",
    "# Map sheet names -> CSV files you’ve been generating\n",
    "CANDIDATE_SHEETS = {\n",
    "    \"benchmarks\":        \"benchmark_results.csv\",           # main table with all query timings\n",
    "    \"insert_throughput\": \"insert_throughput.csv\",\n",
    "    \"table_sizes\":       \"table_sizes.csv\",\n",
    "    \"index_details\":     \"index_details.csv\",\n",
    "    \"indexes_ck\":        \"bench_lines_ck_show_index.csv\",\n",
    "    \"indexes_sk\":        \"bench_lines_sk_show_index.csv\",\n",
    "    \"buffer_pool\":       \"buffer_pool_diffs.csv\",\n",
    "    \"optimize_times\":    \"optimize_times.csv\",\n",
    "    # add more here if you create additional CSVs\n",
    "}\n",
    "\n",
    "# Collect any CSVs that actually exist\n",
    "to_write = {}\n",
    "for sheet, csvfile in CANDIDATE_SHEETS.items():\n",
    "    if csvfile and os.path.exists(csvfile):\n",
    "        to_write[sheet] = pd.read_csv(csvfile)\n",
    "\n",
    "# Optional: a small meta sheet (only adds rows if vars exist)\n",
    "meta_rows = []\n",
    "if \"STORE\" in globals():  meta_rows.append((\"STORE\", STORE))\n",
    "if \"DAY_LO\" in globals(): meta_rows.append((\"DAY_LO\", DAY_LO))\n",
    "if \"DAY_HI\" in globals(): meta_rows.append((\"DAY_HI\", DAY_HI))\n",
    "meta_rows.append((\"GENERATED_AT\", time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "if meta_rows:\n",
    "    to_write[\"meta\"] = pd.DataFrame(meta_rows, columns=[\"key\", \"value\"])\n",
    "\n",
    "# Create the Excel file\n",
    "try:\n",
    "    writer = pd.ExcelWriter(OUT_XLSX, engine=\"xlsxwriter\")\n",
    "    supports_fmt = True\n",
    "except Exception:\n",
    "    # Fallback if xlsxwriter isn’t installed\n",
    "    writer = pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\")\n",
    "    supports_fmt = False\n",
    "\n",
    "for sheet_name, df in to_write.items():\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    if supports_fmt:\n",
    "        ws = writer.sheets[sheet_name]\n",
    "        # freeze header row\n",
    "        ws.freeze_panes(1, 0)\n",
    "        # add autofilter\n",
    "        if df.shape[1] > 0:\n",
    "            ws.autofilter(0, 0, max(len(df), 1), df.shape[1]-1)\n",
    "        # autosize columns (cap width to 60)\n",
    "        for col_idx, col in enumerate(df.columns):\n",
    "            maxlen = max([len(str(col))] + [len(str(v)) for v in df[col].astype(str).tolist()]) if len(df) else len(str(col))\n",
    "            ws.set_column(col_idx, col_idx, min(maxlen + 2, 60))\n",
    "\n",
    "writer.close()\n",
    "print(f\" Wrote {OUT_XLSX} with sheets: {', '.join(to_write.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f451a57",
   "metadata": {},
   "source": [
    "# Executive Summary — 4 Deliverables (Store 140, Days 6..36)\n",
    "\n",
    "| Task              | CK (ms) | SK (ms) | Rows | Faster |\n",
    "|-------------------|--------:|--------:|-----:|:------:|\n",
    "| spend_trends      |   5.47  |   6.63  |  177 | **CK** |\n",
    "| demo_influence    | 563.75  | 488.05  |   10 | **SK** |\n",
    "| growth_categories | 1561.09 | 1247.66 |   10 | **SK** |\n",
    "| campaign_impact   | 6744.19 | 6119.41 |    2 | **SK** |\n",
    "\n",
    "**Summary:** SK wins **3/4** deliverables (joins/aggregations); CK wins **spend_trends** because the query is perfectly aligned with CK’s clustered key `(store, day, product, …)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc5e7b",
   "metadata": {},
   "source": [
    "## Deliverables — What was measured, why, and why each result makes sense\n",
    "\n",
    "### 1) Spend trends — revenue by product (winner: **CK**, 5.47 ms vs 6.63 ms; 177 rows)\n",
    "- **Query shape:** `WHERE store_sk=? AND day BETWEEN ? AND ?  GROUP BY product_sk`\n",
    "- **Why include:** Bread-and-butter retail slice (recent window per store).\n",
    "- **Access path:**  \n",
    "  - **CK:** clustered PRIMARY KEY `(store, day, product, …)` → **perfect left-prefix match** → tight range scan; row already on leaf → minimal lookups.  \n",
    "  - **SK:** secondary `idx_store_day_prod(store, day, product)` → also fast, but one more structure to maintain.\n",
    "- **Why CK won:** Storage order matches the predicate; reads are sequential and naturally “covering” (clustered index holds the row).\n",
    "\n",
    "**If asked “How to make SK equal/better?”** Add/keep `idx_store_day_prod`; consider including `sales` to make it covering (trade: more index bytes).\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Demographic influence — revenue by income × age (winner: **SK**, 488.05 ms vs 563.75 ms; 10 rows)\n",
    "- **Query shape:** scan fact → join `household_dim` by `household_sk` → `GROUP BY income_desc, age_desc` → `ORDER BY revenue DESC LIMIT 10`.\n",
    "- **Why include:** Classic **join + aggregate** used by marketing.\n",
    "- **Access path:** Both designs **scan ~1.42M rows** and do **cheap PK lookups** into the dimension, then aggregate & sort.\n",
    "- **Why SK edged out:** Mostly scan/aggregate dominated; SK’s row layout and narrower secondary index can shave a bit of work. Gap is modest and often sensitive to buffer-pool warm-up.\n",
    "\n",
    "**Tuning levers:** Pre-aggregate by household period; add covering pieces if repeatedly selecting the same columns.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Growth categories — 30-day vs prior 28-day (winner: **SK**, 1,247.66 ms vs 1,561.09 ms; 10 rows)\n",
    "- **Query shape:** two windowed scans on fact → join `product_dim` → compare `SUM(sales)` by category; `ORDER BY` growth.\n",
    "- **Why include:** Common PoP growth analysis.\n",
    "- **Access path:** Two large scans + join + group.  \n",
    "- **Why SK edged out:** Similar reason as #2—slightly less I/O via compact secondary index paths; still largely scan/aggregate bound.\n",
    "\n",
    "**Tuning levers:** Materialize daily category revenue; consider summary tables or column pruning/covering indexes.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Campaign impact — coupon redeemed vs not (winner: **SK**, 6,119.41 ms vs 6,744.19 ms; 2 rows)\n",
    "- **Query shape:** match redemption to fact on `(household, product, day)` → split **redeemed** vs **not_redeemed** → aggregate revenue/units.\n",
    "- **Why include:** Typical marketing effectiveness readout.\n",
    "- **Access path:** Big join + two aggregates (one anti-join).  \n",
    "- **Why SK edged out:** Extra flexibility from secondary indexes; but workload is **data-volume dominated**, so the delta is moderate.\n",
    "\n",
    "**Tuning levers:** Precompute redemption flags per (store, day, product); use temp tables or summaries for frequent reporting windows.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall\n",
    "- “**SK wins 3/4 deliverables** because those are **join/aggregate** heavy and benefit from flexible secondary indexes; **CK wins spend_trends** because that query is **perfectly aligned** to its clustered order `(store → day → product → …)`.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1be3f1",
   "metadata": {},
   "source": [
    "\n",
    "# 10 Benchmark Scenarios (Notebook “parameters”)\n",
    "\n",
    "| # | Scenario / What we measured | Query/Feature exercised | Metric(s) captured | Winner (this run) |\n",
    "|---|-----------------------------|-------------------------|--------------------|-------------------|\n",
    "| 1 | **Spend trends** (store + 30-day window → revenue by product) | Range scan on `(store, day)` + `GROUP BY product` | ms, rowcount | **CK** (5.47 vs 6.63 ms) |\n",
    "| 2 | **Demographic influence** (income × age → revenue) | Scan fact + join `household_dim` + aggregate | ms, rowcount | **SK** (488 vs 564 ms) |\n",
    "| 3 | **Growth categories** (30-day vs prior 28-day) | Two windowed scans + join `product_dim` | ms, rowcount | **SK** (1248 vs 1561 ms) |\n",
    "| 4 | **Campaign impact** (coupon redeemed vs not) | Join fact ↔ redemption + aggregate | ms, rowcount | **SK** (6119 vs 6744 ms) |\n",
    "| 5 | **Household lookup** (`SUM(qty)` by `household_sk`) | CK: no index → scan; SK: `idx_household` | ms, rows examined | **SK** (~7.8 ms vs ~1444 ms) |\n",
    "| 6 | **Store + day micro** (same window as #1) | Pure index range scan timing | ms (top & range node) | **SK** (~26 ms vs ~43 ms) |\n",
    "| 7 | **Insert throughput** (~1.42M rows) | Bulk insert into each design | ms, rows inserted | **CK** (23,434 vs 37,729 ms) |\n",
    "| 8 | **Delete window** (store 140, days 6..36) | Localized range delete | ms, rows deleted | **CK** (91 vs 119 ms) |\n",
    "| 9 | **Storage footprint** | `information_schema.tables` | data_mb, index_mb, total_mb | **CK** (98.7 MB vs 184.8 MB) |\n",
    "|10 | **Maintenance / OPTIMIZE** | Table rebuild + analyze | seconds | **CK** (25 s vs 53 s) |\n",
    "\n",
    "*(Bonus check we also ran: buffer-pool impact — CK created ~26k extra read requests and ~12,490 disk reads vs SK’s ~8k and ~89 for the household lookup; SK is far more cache-friendly for that access pattern.)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
